"""
Elasticsearch ì¬ì¸ë±ì‹± with Semantic Chunking

ëª©ì :
1. ê¸°ì¡´ documents.jsonlì„ Semantic Chunkingìœ¼ë¡œ ì¬ì²˜ë¦¬
2. test ì¸ë±ìŠ¤ ì¬ìƒì„±
3. ì˜ë¯¸ì ìœ¼ë¡œ ì™„ê²°ëœ ì²­í¬ë¡œ ì¬ì¸ë±ì‹±
"""

import json
import time
from elasticsearch import Elasticsearch
from tqdm import tqdm
from semantic_chunking import semantic_sentence_chunking

# ES ì—°ê²°
es = Elasticsearch(['http://localhost:9200'])

def backup_existing_index():
    """ê¸°ì¡´ ì¸ë±ìŠ¤ ë°±ì—…"""
    print(f"\n{'='*80}")
    print("ê¸°ì¡´ ì¸ë±ìŠ¤ ë°±ì—…")
    print(f"{'='*80}")

    if not es.indices.exists(index='test'):
        print("âš ï¸ ê¸°ì¡´ 'test' ì¸ë±ìŠ¤ ì—†ìŒ (ë°±ì—… ë¶ˆí•„ìš”)")
        return

    # test_backupìœ¼ë¡œ ë³µì‚¬
    if es.indices.exists(index='test_backup'):
        print("ğŸ—‘ï¸  ê¸°ì¡´ ë°±ì—… ì‚­ì œ ì¤‘...")
        es.indices.delete(index='test_backup')

    print("ğŸ’¾ ë°±ì—… ìƒì„± ì¤‘... (test â†’ test_backup)")
    es.reindex(
        body={
            'source': {'index': 'test'},
            'dest': {'index': 'test_backup'}
        },
        wait_for_completion=True
    )

    backup_count = es.count(index='test_backup')['count']
    print(f"âœ… ë°±ì—… ì™„ë£Œ: {backup_count}ê°œ ë¬¸ì„œ")

def create_new_index():
    """ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (Nori ë¶„ì„ê¸° í¬í•¨)"""
    print(f"\n{'='*80}")
    print("ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
    print(f"{'='*80}")

    # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
    if es.indices.exists(index='test'):
        print("ğŸ—‘ï¸  ê¸°ì¡´ 'test' ì¸ë±ìŠ¤ ì‚­ì œ ì¤‘...")
        es.indices.delete(index='test')

    # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    print("ğŸ—ï¸  ìƒˆ 'test' ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    es.indices.create(
        index='test',
        body={
            'settings': {
                'number_of_shards': 1,
                'number_of_replicas': 0,
                'analysis': {
                    'analyzer': {
                        'nori': {
                            'type': 'custom',
                            'tokenizer': 'nori_tokenizer',
                            'filter': ['nori_part_of_speech']
                        }
                    }
                }
            },
            'mappings': {
                'properties': {
                    'docid': {'type': 'keyword'},
                    'original_docid': {'type': 'keyword'},  # ì›ë³¸ ë¬¸ì„œ ID
                    'chunk_index': {'type': 'integer'},  # ì²­í¬ ìˆœì„œ
                    'total_chunks': {'type': 'integer'},  # ì „ì²´ ì²­í¬ ìˆ˜
                    'content': {
                        'type': 'text',
                        'analyzer': 'nori'
                    }
                }
            }
        }
    )

    print("âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")

def reindex_with_semantic_chunking(documents_path='../data/documents.jsonl'):
    """Semantic Chunking ê¸°ë°˜ ì¬ì¸ë±ì‹±"""
    print(f"\n{'='*80}")
    print("Semantic Chunking ì¬ì¸ë±ì‹±")
    print(f"{'='*80}")

    # ë¬¸ì„œ ë¡œë“œ
    print(f"\nğŸ“‚ ë¬¸ì„œ ë¡œë“œ ì¤‘: {documents_path}")
    with open(documents_path, 'r', encoding='utf-8') as f:
        documents = [json.loads(line) for line in f]

    print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")

    # í†µê³„
    total_chunks = 0
    chunk_distribution = {}  # ì²­í¬ ìˆ˜ë³„ ë¬¸ì„œ ê°œìˆ˜
    total_original_docs = 0
    total_kept_original = 0  # ì²­í‚¹ë˜ì§€ ì•Šì€ ë¬¸ì„œ (1ê°œ ì²­í¬)
    total_chunked = 0  # ì²­í‚¹ëœ ë¬¸ì„œ (2ê°œ ì´ìƒ)

    # ì¬ì¸ë±ì‹±
    print(f"\nğŸ”„ ì¬ì¸ë±ì‹± ì¤‘...")
    for doc in tqdm(documents, desc="Semantic Chunking"):
        docid = doc['docid']
        content = doc['content']
        total_original_docs += 1

        # Semantic Chunking with Overlap
        chunks = semantic_sentence_chunking(
            content,
            max_chunk_size=400,
            min_chunk_size=100,
            similarity_threshold=0.65,
            overlap_sentences=1  # 1ë¬¸ì¥ overlap
        )

        # í†µê³„ ìˆ˜ì§‘
        num_chunks = len(chunks)
        chunk_distribution[num_chunks] = chunk_distribution.get(num_chunks, 0) + 1
        total_chunks += num_chunks

        if num_chunks == 1:
            total_kept_original += 1
        else:
            total_chunked += 1

        # ê° ì²­í¬ ì¸ë±ì‹±
        for i, chunk in enumerate(chunks):
            # DocID ìƒì„± ê·œì¹™:
            # - 1ê°œ ì²­í¬: ì›ë³¸ docid ê·¸ëŒ€ë¡œ (í˜¸í™˜ì„±)
            # - 2ê°œ ì´ìƒ: docid_chunk0, docid_chunk1, ...
            if num_chunks == 1:
                chunk_docid = docid
            else:
                chunk_docid = f"{docid}_chunk{i}"

            es.index(
                index='test',
                body={
                    'docid': chunk_docid,
                    'original_docid': docid,
                    'chunk_index': i,
                    'total_chunks': num_chunks,
                    'content': chunk
                }
            )

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*80}")
    print("ì¬ì¸ë±ì‹± ì™„ë£Œ")
    print(f"{'='*80}")

    print(f"\nğŸ“Š í†µê³„:")
    print(f"  ì›ë³¸ ë¬¸ì„œ: {total_original_docs}ê°œ")
    print(f"  ì „ì²´ ì²­í¬: {total_chunks}ê°œ")
    print(f"  í‰ê·  ì²­í¬ ìˆ˜: {total_chunks/total_original_docs:.2f}ê°œ/ë¬¸ì„œ")

    print(f"\nğŸ“ˆ ì²­í¬ ë¶„í¬:")
    for num_chunks in sorted(chunk_distribution.keys()):
        count = chunk_distribution[num_chunks]
        pct = count / total_original_docs * 100
        print(f"  {num_chunks}ê°œ ì²­í¬: {count:4d}ê°œ ë¬¸ì„œ ({pct:5.1f}%)")

    print(f"\nğŸ” ì²­í‚¹ íš¨ê³¼:")
    print(f"  ì²­í‚¹ ì•ˆ ë¨ (1ê°œ): {total_kept_original:4d}ê°œ ({total_kept_original/total_original_docs*100:5.1f}%)")
    print(f"  ì²­í‚¹ ë¨ (2ê°œ+): {total_chunked:4d}ê°œ ({total_chunked/total_original_docs*100:5.1f}%)")

    # Elasticsearch ë¬¸ì„œ ìˆ˜ í™•ì¸
    es.indices.refresh(index='test')
    time.sleep(2)
    indexed_count = es.count(index='test')['count']

    print(f"\nâœ… Elasticsearch ì¸ë±ì‹± ì™„ë£Œ: {indexed_count}ê°œ ë¬¸ì„œ")

    if indexed_count != total_chunks:
        print(f"âš ï¸ ê²½ê³ : ì˜ˆìƒ ì²­í¬ ìˆ˜({total_chunks})ì™€ ì¸ë±ì‹± ìˆ˜({indexed_count}) ë¶ˆì¼ì¹˜")
    else:
        print(f"âœ… ê²€ì¦ ì„±ê³µ: ì²­í¬ ìˆ˜ ì¼ì¹˜")

def verify_index():
    """ì¸ë±ìŠ¤ ê²€ì¦"""
    print(f"\n{'='*80}")
    print("ì¸ë±ìŠ¤ ê²€ì¦")
    print(f"{'='*80}")

    # ìƒ˜í”Œ ê²€ìƒ‰
    response = es.search(
        index='test',
        body={
            'query': {'match': {'content': 'DNA'}},
            'size': 3
        }
    )

    print(f"\nğŸ” ìƒ˜í”Œ ê²€ìƒ‰ (query='DNA'):")
    print(f"  ë§¤ì¹­ ë¬¸ì„œ: {response['hits']['total']['value']}ê°œ")

    print(f"\n  Top-3:")
    for i, hit in enumerate(response['hits']['hits'], 1):
        source = hit['_source']
        print(f"\n  [{i}] {source['docid']}")
        print(f"      ì›ë³¸: {source['original_docid']}")
        print(f"      ì²­í¬: {source['chunk_index']+1}/{source['total_chunks']}")
        print(f"      ë‚´ìš©: {source['content'][:80]}...")

def main():
    print("="*80)
    print("Elasticsearch Overlap Chunking ì¬ì¸ë±ì‹±")
    print("="*80)

    # 1. ES ì—°ê²° í™•ì¸
    if not es.ping():
        print("âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨")
        print("   docker start elasticsearch ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        return

    print("âœ… Elasticsearch ì—°ê²° ì„±ê³µ")

    # 2. ê¸°ì¡´ ì¸ë±ìŠ¤ ë°±ì—…
    backup_existing_index()

    # 3. ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
    create_new_index()

    # 4. Semantic Chunking ì¬ì¸ë±ì‹±
    reindex_with_semantic_chunking()

    # 5. ê²€ì¦
    verify_index()

    print(f"\n{'='*80}")
    print("âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
    print(f"{'='*80}")

    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. solar_semantic_v1.py ì‹¤í–‰ìœ¼ë¡œ ìƒˆ ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸")
    print(f"  2. ê²°ê³¼ê°€ ì¢‹ìœ¼ë©´ ìœ ì§€, ë‚˜ì˜ë©´ ë¡¤ë°±:")
    print(f"     ë¡¤ë°± ëª…ë ¹: python rollback_index.py")

if __name__ == "__main__":
    main()
