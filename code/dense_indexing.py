"""
Dense Retrievalì„ ìœ„í•œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ë° ì¸ë±ì‹±
ko-sroberta ëª¨ë¸ ì‚¬ìš©
"""

import json
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from elasticsearch import Elasticsearch
import pickle

# ES ì—°ê²°
es = Elasticsearch(['http://localhost:9200'])

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ semantic_chunking.pyì—ì„œ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸)
print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model.get_sentence_embedding_dimension()}ì°¨ì›")

def generate_embeddings_for_index(index_name='test', batch_size=32):
    """
    ES ì¸ë±ìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ ì„ë² ë”© ìƒì„±

    Args:
        index_name: Elasticsearch ì¸ë±ìŠ¤ëª…
        batch_size: ë°°ì¹˜ í¬ê¸°
    """
    print(f"\n{'='*80}")
    print(f"Dense Indexing: {index_name}")
    print(f"{'='*80}")

    # ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
    count_response = es.count(index=index_name)
    total_docs = count_response['count']
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ")

    # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (scroll API ì‚¬ìš©)
    documents = []
    scroll_resp = es.search(
        index=index_name,
        scroll='5m',
        size=1000,
        body={
            'query': {'match_all': {}},
            '_source': ['docid', 'content', 'original_docid']
        }
    )

    scroll_id = scroll_resp['_scroll_id']
    documents.extend(scroll_resp['hits']['hits'])

    # ìŠ¤í¬ë¡¤ë¡œ ë‚˜ë¨¸ì§€ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    while len(scroll_resp['hits']['hits']) > 0:
        scroll_resp = es.scroll(scroll_id=scroll_id, scroll='5m')
        documents.extend(scroll_resp['hits']['hits'])
        print(f"ì§„í–‰: {len(documents)}/{total_docs}ê°œ ë¬¸ì„œ ë¡œë“œë¨")

    es.clear_scroll(scroll_id=scroll_id)

    print(f"\nâœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

    # ì„ë² ë”© ìƒì„±
    print(f"\nì„ë² ë”© ìƒì„± ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")

    embeddings_dict = {}
    contents = []
    docids = []

    for doc in documents:
        source = doc['_source']
        docid = source['docid']
        content = source['content']

        docids.append(docid)
        contents.append(content)

    # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
    all_embeddings = []
    for i in tqdm(range(0, len(contents), batch_size), desc="ì„ë² ë”© ìƒì„±"):
        batch = contents[i:i+batch_size]
        embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=False)
        all_embeddings.extend(embeddings)

    # Dictionary ìƒì„±
    for docid, embedding in zip(docids, all_embeddings):
        embeddings_dict[docid] = embedding

    # ì €ì¥
    output_file = f'embeddings_{index_name}.pkl'
    with open(output_file, 'wb') as f:
        pickle.dump(embeddings_dict, f)

    print(f"\n{'='*80}")
    print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_file}")
    print(f"ğŸ“Š í†µê³„:")
    print(f"   ë¬¸ì„œ ìˆ˜: {len(embeddings_dict)}ê°œ")
    print(f"   ì„ë² ë”© ì°¨ì›: {all_embeddings[0].shape[0]}ì°¨ì›")
    print(f"   íŒŒì¼ í¬ê¸°: {len(pickle.dumps(embeddings_dict)) / (1024*1024):.1f} MB")
    print(f"{'='*80}")

    return embeddings_dict

def test_dense_search(embeddings_dict, query, top_k=10):
    """
    í…ŒìŠ¤íŠ¸ìš© Dense ê²€ìƒ‰

    Args:
        embeddings_dict: {docid: embedding} ë”•ì…”ë„ˆë¦¬
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
    """
    # ì¿¼ë¦¬ ì„ë² ë”©
    query_emb = model.encode([query], convert_to_numpy=True)[0]

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    scores = []
    for docid, doc_emb in embeddings_dict.items():
        similarity = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
        scores.append((docid, similarity))

    # ì •ë ¬
    scores.sort(key=lambda x: x[1], reverse=True)

    return scores[:top_k]

if __name__ == "__main__":
    # ì„ë² ë”© ìƒì„±
    embeddings = generate_embeddings_for_index('test', batch_size=64)

    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("Dense Search í…ŒìŠ¤íŠ¸")
    print("="*80)

    test_queries = [
        "ê´‘í•©ì„±ì˜ ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€?",
        "DNA ë³µì œ ê³¼ì •ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "íƒœì–‘ê³„ í–‰ì„±ì˜ íŠ¹ì§•ì€?"
    ]

    for query in test_queries:
        print(f"\nì¿¼ë¦¬: {query}")
        results = test_dense_search(embeddings, query, top_k=3)
        for i, (docid, score) in enumerate(results, 1):
            print(f"  {i}. {docid} (ìœ ì‚¬ë„: {score:.4f})")

    print("\n" + "="*80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)
