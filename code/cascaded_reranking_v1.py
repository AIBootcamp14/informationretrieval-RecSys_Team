"""
Cascaded Reranking v1: 2-Stage LLM Reranking
- Stage 1: Top 30 â†’ Top 10 (ë¹ ë¥¸ í•„í„°ë§)
- Stage 2: Top 10 â†’ Top 3 (ì •ë°€í•œ Reranking)
- LLM ê¸°ë°˜ ì¼ë°˜ëŒ€í™” ìë™ ë¶„ë¥˜ (í•˜ë“œì½”ë”© ì œê±°)

ëª©í‘œ: "Lost in the Middle" ë¬¸ì œ í•´ê²°í•˜ë©° Recall í–¥ìƒ
ì˜ˆìƒ ì„±ëŠ¥: 0.81~0.82 MAP@3

ì—…ë°ì´íŠ¸ (2025-11-24):
- í•˜ë“œì½”ë”©ëœ ì¼ë°˜ëŒ€í™” ID ì œê±°
- LLM ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ë¡œ ë³€ê²½ (ê·œì¹™ + Solar Pro)
- ì–´ë–¤ í‰ê°€ ë°ì´í„°ì—ë„ ëŒ€ì‘ ê°€ëŠ¥
"""

import json
import os
import pickle
import numpy as np
from tqdm import tqdm
from elasticsearch import Elasticsearch
from FlagEmbedding import BGEM3FlagModel
from openai import OpenAI

# ES ì—°ê²°
es = Elasticsearch(['http://localhost:9200'])

# Solar API ì´ˆê¸°í™”
upstage_api_key = os.environ.get('UPSTAGE_API_KEY')
client = None
if upstage_api_key:
    client = OpenAI(
        api_key=upstage_api_key,
        base_url="https://api.upstage.ai/v1/solar"
    )

# BGE-M3 ëª¨ë¸ ë¡œë“œ
print("BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
print("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# BGE-M3 ìµœì í™” ì„ë² ë”© ë¡œë“œ
print("\nBGE-M3 ìµœì í™” ì„ë² ë”© ë¡œë“œ ì¤‘...")
with open('embeddings_test_bgem3_optimized.pkl', 'rb') as f:
    embeddings_dict = pickle.load(f)
print(f"âœ… {len(embeddings_dict)}ê°œ ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")

def is_smalltalk(query, client=None):
    """
    LLM ê¸°ë°˜ ì¼ë°˜ëŒ€í™” ìë™ ë¶„ë¥˜

    í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹:
    1. ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•œ ì¼€ì´ìŠ¤ ë¨¼ì € ì²˜ë¦¬ (ë¹ ë¦„)
    2. ì• ë§¤í•œ ê²½ìš°ë§Œ LLM í˜¸ì¶œ (ì •í™•í•¨)

    Returns:
        True: ì¼ë°˜ëŒ€í™” (ë¬¸ì„œ ê²€ìƒ‰ ë¶ˆí•„ìš”)
        False: ê³¼í•™ì§ˆë¬¸ (ë¬¸ì„œ ê²€ìƒ‰ í•„ìš”)
    """
    # 1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ ëª…í™•í•œ ì¼€ì´ìŠ¤

    # ë§¤ìš° ì§§ì€ ë¬¸ì¥ (5ì ì´í•˜)
    if len(query) < 5:
        return True

    # ëª…í™•í•œ ì¸ì‚¬ (ì§§ì€ ë¬¸ì¥ + ì¸ì‚¬ í‚¤ì›Œë“œ)
    greetings = ['ì•ˆë…•', 'ë°˜ê°€ì›Œ', 'ë°˜ê°‘', 'hi', 'hello', 'hey', 'ì˜ê°€', 'bye']
    if any(word in query for word in greetings) and len(query) < 15:
        return True

    # ëª…í™•í•œ ê°ì • í‘œí˜„ (ì§§ì€ ë¬¸ì¥ + ê°ì • í‚¤ì›Œë“œ)
    emotions = ['ê³ ë§ˆì›Œ', 'ê°ì‚¬', 'ìˆ˜ê³ ', 'ë¯¸ì•ˆ', 'ì£„ì†¡', 'ì‹ ë‚˜', 'í˜ë“¤', 'ìŠ¬í”„', 'ê¸°ë»']
    if any(word in query for word in emotions) and len(query) < 20:
        return True

    # ëª…í™•í•œ ì§ˆë¬¸ ë§ˆì»¤ (ê³¼í•™ì§ˆë¬¸ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
    question_markers = ['ì™œ', 'ì–´ë–»ê²Œ', 'ë¬´ì—‡', 'ë­', 'ì–´ë””', 'ì–¸ì œ', 'ëˆ„êµ¬', 'ì–´ëŠ', 'ì›ë¦¬', 'ì´ìœ ', 'ë°©ë²•', 'ê³¼ì •']
    if any(marker in query for marker in question_markers):
        return False

    # ì§ˆë¬¸ ë¶€í˜¸
    if '?' in query or 'ï¼Ÿ' in query:
        return False

    # 2ë‹¨ê³„: ì• ë§¤í•œ ì¼€ì´ìŠ¤ë§Œ LLM í˜¸ì¶œ
    if not client:
        # LLMì´ ì—†ìœ¼ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ ê³¼í•™ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        return False

    try:
        prompt = f"""ë‹¤ìŒ ë¬¸ì¥ì´ ê³¼í•™/ê¸°ìˆ /ì§€ì‹ì— ëŒ€í•œ ì§ˆë¬¸ì¸ì§€, ì¼ë°˜ì ì¸ ëŒ€í™”(ì¸ì‚¬, ê°ì •í‘œí˜„, ì¡ë‹´)ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë¬¸ì¥: {query}

ê³¼í•™ì§ˆë¬¸ì´ë©´ "SCIENCE", ì¼ë°˜ëŒ€í™”ë©´ "SMALLTALK"ë¡œë§Œ ë‹µí•˜ì„¸ìš”."""

        response = client.chat.completions.create(
            model="solar-pro",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=10
        )

        result = response.choices[0].message.content.strip().upper()
        return "SMALLTALK" in result

    except Exception as e:
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ê³¼í•™ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬
        print(f"âš ï¸  Smalltalk ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        return False

def rewrite_query_with_context(msg):
    """
    ë©€í‹°í„´ ëŒ€í™”ì˜ ë§¥ë½ì„ í†µí•©í•˜ì—¬ ì¿¼ë¦¬ ì¬ì‘ì„± (rag_with_context.py ê¸°ë°˜)
    """
    if isinstance(msg, str):
        return msg

    if len(msg) == 1:
        return msg[0]['content']

    current_query = msg[-1]['content']

    # ëŒ€ëª…ì‚¬ë‚˜ ëª¨í˜¸í•œ í‘œí˜„ í™•ì¸
    ambiguous_terms = ['ê·¸ ', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì´ê±°', 'ì €ê²ƒ', 'ì €ê±°', 'ì™œ', 'ì–´ë–»ê²Œ', 'ì´ìœ ']

    if not any(term in current_query for term in ambiguous_terms):
        return current_query

    if not client:
        return current_query

    # LLMìœ¼ë¡œ ì¿¼ë¦¬ ì¬ì‘ì„±
    conversation_context = "\n".join([
        f"{m['role']}: {m['content']}" for m in msg[:-1]
    ])

    prompt = f"""ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_context}

í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{current_query}"

ì´ ì§ˆë¬¸ì„ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
ëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ ë“±)ë¥¼ êµ¬ì²´ì ì¸ ëª…ì‚¬ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.

ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="solar-pro",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=150
        )

        rewritten = response.choices[0].message.content.strip()
        rewritten = rewritten.strip('"').strip("'")

        return rewritten

    except Exception as e:
        return current_query

def search_bm25(query, top_k=20):
    """BM25 ê²€ìƒ‰"""
    fetch_size = top_k + 5

    response = es.search(
        index='test',
        body={
            'query': {
                'match': {
                    'content': {
                        'query': query,
                        'analyzer': 'nori'
                    }
                }
            },
            'size': fetch_size
        }
    )

    if not response['hits']['hits']:
        return []

    # original_docid ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    seen_original_docids = set()
    results = []

    for hit in response['hits']['hits']:
        source = hit['_source']
        original_docid = source.get('original_docid', source['docid'])

        if original_docid in seen_original_docids:
            continue

        seen_original_docids.add(original_docid)
        results.append({
            'docid': original_docid,
            'content': source['content'],
            'score': hit['_score'],
            'source': 'bm25'
        })

        if len(results) >= top_k:
            break

    return results

def bgem3_hybrid_score(query_dense, query_sparse, query_colbert,
                       doc_dense, doc_sparse, doc_colbert,
                       w1=0.4, w2=0.3, w3=0.3):
    """BGE-M3 Hybrid Scoring"""
    # 1. Dense ìœ ì‚¬ë„
    s_dense = np.dot(query_dense, doc_dense) / (
        np.linalg.norm(query_dense) * np.linalg.norm(doc_dense)
    )

    # 2. Sparse ìœ ì‚¬ë„
    s_lex = 0.0
    if query_sparse and doc_sparse:
        common_tokens = set(query_sparse.keys()) & set(doc_sparse.keys())
        for token in common_tokens:
            s_lex += query_sparse[token] * doc_sparse[token]

    # 3. ColBERT ìœ ì‚¬ë„
    s_mul = 0.0
    if query_colbert.shape[0] > 0 and doc_colbert.shape[0] > 0:
        query_colbert_norm = query_colbert / np.linalg.norm(query_colbert, axis=1, keepdims=True)
        doc_colbert_norm = doc_colbert / np.linalg.norm(doc_colbert, axis=1, keepdims=True)
        sim_matrix = np.dot(query_colbert_norm, doc_colbert_norm.T)
        s_mul = np.mean(np.max(sim_matrix, axis=1))

    hybrid_score = w1 * s_dense + w2 * s_lex + w3 * s_mul
    return hybrid_score

def search_bgem3_hybrid(query, embeddings_dict, top_k=20, max_length=128):
    """
    BGE-M3 Hybrid ê²€ìƒ‰ (ì¿¼ë¦¬ ê¸¸ì´ ìµœì í™”)
    max_length: 64 â†’ 128 (Query Expansionìœ¼ë¡œ ì¿¼ë¦¬ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŒ)
    """
    # BGE-M3 ì¿¼ë¦¬ ì„ë² ë”© (ìµœì í™”ëœ max_length)
    query_embedding = model.encode(
        [query],
        return_dense=True,
        return_sparse=True,
        return_colbert_vecs=True,
        max_length=max_length
    )

    query_dense = query_embedding['dense_vecs'][0]
    query_sparse = query_embedding['lexical_weights'][0]
    query_colbert = query_embedding['colbert_vecs'][0]

    # ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ Hybrid Score ê³„ì‚°
    scores = []
    for docid, doc_emb in embeddings_dict.items():
        score = bgem3_hybrid_score(
            query_dense, query_sparse, query_colbert,
            doc_emb['dense'], doc_emb['sparse'], doc_emb['colbert']
        )
        scores.append((docid, score))

    # ì •ë ¬
    scores.sort(key=lambda x: x[1], reverse=True)

    # ESì—ì„œ content ê°€ì ¸ì˜¤ê¸°
    results = []
    for docid, score in scores[:top_k]:
        try:
            resp = es.search(
                index='test',
                body={
                    'query': {
                        'bool': {
                            'should': [
                                {'term': {'docid.keyword': docid}},
                                {'term': {'original_docid.keyword': docid}}
                            ]
                        }
                    },
                    'size': 1
                }
            )

            if resp['hits']['hits']:
                source = resp['hits']['hits'][0]['_source']
                results.append({
                    'docid': docid,
                    'content': source['content'],
                    'score': float(score),
                    'source': 'bgem3_hybrid'
                })
        except Exception as e:
            continue

    return results

def hybrid_search_rrf(query, embeddings_dict, top_k=30, k=60, query_max_length=128):
    """
    RRFë¡œ BM25 + BGE-M3 Hybrid ê²°í•©
    top_kë¥¼ 30ìœ¼ë¡œ ì¦ê°€ (Cascaded Rerankingìš©)
    """
    # BM25 ê²€ìƒ‰
    bm25_results = search_bm25(query, top_k=top_k)

    # BGE-M3 Hybrid ê²€ìƒ‰ (ìµœì í™”ëœ ì¿¼ë¦¬ ê¸¸ì´)
    bgem3_results = search_bgem3_hybrid(query, embeddings_dict, top_k=top_k, max_length=query_max_length)

    # RRF ìŠ¤ì½”ì–´ ê³„ì‚°
    rrf_scores = {}
    doc_contents = {}

    for rank, doc in enumerate(bm25_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + 1 / (k + rank)
        doc_contents[docid] = doc['content']

    for rank, doc in enumerate(bgem3_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + 1 / (k + rank)
        doc_contents[docid] = doc['content']

    # ì •ë ¬
    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

    results = []
    for docid, score in sorted_docs:
        results.append({
            'docid': docid,
            'content': doc_contents[docid],
            'score': score
        })

    return results

def llm_stage1_filter(query, docs, top_k=10):
    """
    Stage 1: Top 30 â†’ Top 10 í•„í„°ë§
    ë¹ ë¥´ê³  ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ í›„ë³´ ì••ì¶•
    """
    if not docs or len(docs) <= top_k or not client:
        return [i for i in range(min(top_k, len(docs)))]

    try:
        doc_list = []
        for i, doc in enumerate(docs[:30]):
            content_preview = doc['content'][:200]
            if len(doc['content']) > 200:
                content_preview += "..."
            doc_list.append(f"[{i}] {content_preview}")

        docs_text = "\n\n".join(doc_list)

        response = client.chat.completions.create(
            model="solar-pro",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ê³¼í•™ ì§€ì‹ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ 1ì°¨ í•„í„°ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {
                    "role": "user",
                    "content": f"""ì¿¼ë¦¬: {query}

ë¬¸ì„œë“¤:
{docs_text}

ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ ìƒìœ„ {top_k}ê°œ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.

ì¶œë ¥: ë²ˆí˜¸ë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„ (ì˜ˆ: 0,2,4,5,7,8,10,12,14,16)
ì„¤ëª… ì—†ì´ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
                }
            ],
            temperature=0.0,
            max_tokens=50
        )

        result = response.choices[0].message.content.strip()
        indices = [int(x.strip()) for x in result.split(',') if x.strip().isdigit()]

        # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ë°˜í™˜
        valid_indices = [idx for idx in indices[:top_k] if 0 <= idx < len(docs)]

        # ë¶€ì¡±í•˜ë©´ ì›ë˜ ìˆœì„œë¡œ ì±„ìš°ê¸°
        for i in range(len(docs)):
            if len(valid_indices) >= top_k:
                break
            if i not in valid_indices:
                valid_indices.append(i)

        return valid_indices[:top_k]

    except Exception as e:
        return [i for i in range(min(top_k, len(docs)))]

def llm_stage2_rerank(query, docs, top_k=3):
    """
    Stage 2: Top 10 â†’ Top 3 ì •ë°€ Reranking
    ê¸°ì¡´ llm_rerankì™€ ë™ì¼í•œ ì •ë°€í•œ í”„ë¡¬í”„íŠ¸
    """
    if not docs or len(docs) <= top_k or not client:
        return [doc['docid'] for doc in docs[:top_k]]

    try:
        doc_list = []
        for i, doc in enumerate(docs):
            content_preview = doc['content'][:300]
            if len(doc['content']) > 300:
                content_preview += "..."
            doc_list.append(f"[{i}] {content_preview}")

        docs_text = "\n\n".join(doc_list)

        response = client.chat.completions.create(
            model="solar-pro",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ê³¼í•™ ì§€ì‹ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ relevance íŒë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì •í™•íˆ ì„ íƒí•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"""ì¿¼ë¦¬: {query}

ë¬¸ì„œë“¤:
{docs_text}

ì´ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ {top_k}ê°œì˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.
- ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ”ê°€?
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ê°€?
- ê³¼í•™ì ìœ¼ë¡œ ì •í™•í•œê°€?

ì¶œë ¥: ë²ˆí˜¸ë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„ (ì˜ˆ: 0,2,4)
ì„¤ëª… ì—†ì´ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
                }
            ],
            temperature=0.0,
            max_tokens=30
        )

        result = response.choices[0].message.content.strip()
        indices = [int(x.strip()) for x in result.split(',') if x.strip().isdigit()]

        reranked_docids = []
        for idx in indices[:top_k]:
            if 0 <= idx < len(docs):
                reranked_docids.append(docs[idx]['docid'])

        # ë¶€ì¡±í•˜ë©´ ì›ë˜ ìˆœì„œë¡œ ì±„ìš°ê¸°
        for doc in docs:
            if len(reranked_docids) >= top_k:
                break
            if doc['docid'] not in reranked_docids:
                reranked_docids.append(doc['docid'])

        return reranked_docids[:top_k]

    except Exception as e:
        return [doc['docid'] for doc in docs[:top_k]]

def llm_cascaded_rerank(query, docs, top_k=3):
    """
    Cascaded (2-Stage) Reranking

    Stage 1: Top 30 â†’ Top 10 (ë¹ ë¥¸ í•„í„°ë§)
    Stage 2: Top 10 â†’ Top 3 (ì •ë°€í•œ íŒë‹¨)
    """
    if not docs or len(docs) <= top_k:
        return [doc['docid'] for doc in docs[:top_k]]

    # Stage 1: Top 30 â†’ Top 10 í•„í„°ë§
    stage1_indices = llm_stage1_filter(query, docs, top_k=10)

    # Stage 1ì—ì„œ ì„ íƒëœ ë¬¸ì„œë“¤
    stage1_docs = [docs[i] for i in stage1_indices if i < len(docs)]

    # Stage 2: Top 10 â†’ Top 3 ì •ë°€ Reranking
    final_docids = llm_stage2_rerank(query, stage1_docs, top_k=top_k)

    return final_docids

def cascaded_reranking_strategy(eval_id, msg, embeddings_dict):
    """
    Cascaded Reranking v1 ì „ëµ

    1. LLM ì¿¼ë¦¬ ì¬ì‘ì„± (ë©€í‹°í„´ ëŒ€í™” ë§¥ë½ í†µí•©)
    2. Hybrid Search (Top 30ìœ¼ë¡œ í™•ì¥)
    3. Cascaded LLM Reranking (30 â†’ 10 â†’ 3)
    """
    # Step 1: ì¿¼ë¦¬ ì¬ì‘ì„±
    rewritten_query = rewrite_query_with_context(msg)

    # ì¼ë°˜ ëŒ€í™” ìë™ ë¶„ë¥˜ (LLM ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ)
    if is_smalltalk(rewritten_query, client):
        return []

    # Step 2: Hybrid Search (Top 30ìœ¼ë¡œ í™•ì¥)
    hybrid_results = hybrid_search_rrf(
        rewritten_query,
        embeddings_dict,
        top_k=30,  # âœ… 15 â†’ 30ìœ¼ë¡œ í™•ì¥
        k=60,
        query_max_length=128
    )

    if not hybrid_results:
        return []

    # Step 3: Cascaded LLM Reranking (30 â†’ 10 â†’ 3)
    final_topk = llm_cascaded_rerank(rewritten_query, hybrid_results, top_k=3)

    return final_topk

def run_experiment():
    """Cascaded Reranking v1 ì‹¤í—˜ ì‹¤í–‰"""
    print("="*80)
    print("Cascaded Reranking v1 ì‹¤í—˜")
    print("="*80)
    print("Stage 1: Top 30 â†’ Top 10 í•„í„°ë§")
    print("Stage 2: Top 10 â†’ Top 3 ì •ë°€ Reranking")
    print("="*80)

    # Eval ë°ì´í„° ë¡œë“œ
    with open('../data/eval.jsonl', 'r', encoding='utf-8') as f:
        eval_data = [json.loads(line) for line in f]

    print(f"\nğŸ“‹ ì´ {len(eval_data)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘\n")

    results = []

    for item in tqdm(eval_data, desc="Cascaded Reranking v1"):
        eval_id = item['eval_id']
        msg = item['msg']

        # Cascaded Reranking ì „ëµ ì‹¤í–‰
        topk = cascaded_reranking_strategy(eval_id, msg, embeddings_dict)

        results.append({
            'eval_id': eval_id,
            'retrieve': topk
        })

    # ì œì¶œ íŒŒì¼ ìƒì„±
    output_path = 'cascaded_reranking_v1_submission.csv'
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in results:
            json_obj = {
                'eval_id': r['eval_id'],
                'topk': r['retrieve']
            }
            f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')

    print(f"\n{'='*80}")
    print(f"âœ… ì‹¤í—˜ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ğŸ’¾ ì œì¶œ íŒŒì¼: {output_path}")
    print(f"{'='*80}")

    return results

if __name__ == "__main__":
    print("\n" + "="*80)
    print("Cascaded Reranking v1: 2-Stage LLM Reranking")
    print("ëª©í‘œ: Lost in the Middle í•´ê²° + Recall í–¥ìƒ")
    print("ì˜ˆìƒ ì„±ëŠ¥: 0.81~0.82 MAP@3")
    print("="*80)
    run_experiment()
