"""
BGE-M3 ìµœì í™” ì„ë² ë”© ìƒì„±
ì„¸ ê°€ì§€ ê²€ìƒ‰ ëª¨ë“œ ëª¨ë‘ í™œì„±í™”: Dense + Sparse + ColBERT
ëª©í‘œ: 0.79+ MAP@3 ë‹¬ì„±ì„ ìœ„í•œ ìµœì í™”
"""

import os
import json
import pickle
import numpy as np
from tqdm import tqdm
from elasticsearch import Elasticsearch
from FlagEmbedding import BGEM3FlagModel

# ES ì—°ê²°
es = Elasticsearch(['http://localhost:9200'])

# BGE-M3 ëª¨ë¸ ë¡œë“œ
print("BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ESì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
print("\nElasticsearchì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")

# Scroll APIë¡œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
query = {
    "query": {"match_all": {}},
    "size": 1000
}

# ì´ˆê¸° ê²€ìƒ‰
response = es.search(index='test', body=query, scroll='5m')
scroll_id = response['_scroll_id']
hits = response['hits']['hits']

all_docs = {}
for hit in hits:
    source = hit['_source']
    # original_docidê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ docid ì‚¬ìš©
    docid = source.get('original_docid', source['docid'])
    content = source['content']

    # ì¤‘ë³µ ì œê±° (original_docid ê¸°ì¤€)
    if docid not in all_docs:
        all_docs[docid] = content

# ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤ ìŠ¤í¬ë¡¤ë¡œ ê°€ì ¸ì˜¤ê¸°
while len(hits) > 0:
    response = es.scroll(scroll_id=scroll_id, scroll='5m')
    scroll_id = response['_scroll_id']
    hits = response['hits']['hits']

    for hit in hits:
        source = hit['_source']
        docid = source.get('original_docid', source['docid'])
        content = source['content']

        if docid not in all_docs:
            all_docs[docid] = content

# ìŠ¤í¬ë¡¤ ì •ë¦¬
es.clear_scroll(scroll_id=scroll_id)

print(f"âœ… {len(all_docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

# BGE-M3ë¡œ ìµœì í™”ëœ ì„ë² ë”© ìƒì„±
print("\n" + "="*80)
print("BGE-M3 ìµœì í™” ì„ë² ë”© ìƒì„±")
print("="*80)
print("ìµœì í™” ì„¤ì •:")
print("  - return_dense=True      (1024d ë²¡í„°)")
print("  - return_sparse=True     (í•™ìŠµëœ ì–´íœ˜ ê°€ì¤‘ì¹˜, BM25ë³´ë‹¤ ìš°ìˆ˜)")
print("  - return_colbert_vecs=True (ë‹¤ì¤‘ ë²¡í„°, ì •ë°€í•œ ê´€ë ¨ì„±)")
print("  - max_length=1024        (ê³¼í•™ ë¬¸ì„œì— ìµœì í™”)")
print("  - batch_size=12          (ë©”ëª¨ë¦¬ ê³ ë ¤)")
print("="*80 + "\n")

embeddings_dict = {}

batch_size = 12  # ColBERT ë²¡í„°ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤
docids = list(all_docs.keys())
contents = [all_docs[docid] for docid in docids]

for i in tqdm(range(0, len(docids), batch_size), desc="ì„ë² ë”© ìƒì„±"):
    batch_docids = docids[i:i+batch_size]
    batch_contents = contents[i:i+batch_size]

    try:
        # BGE-M3 ìµœì í™” ì„ë² ë”© (ì„¸ ê°€ì§€ ëª¨ë“œ ëª¨ë‘ í™œì„±í™”)
        embeddings = model.encode(
            batch_contents,
            return_dense=True,        # âœ… Dense ë²¡í„° (1024d)
            return_sparse=True,       # âœ… Sparse ê°€ì¤‘ì¹˜ (BM25 ëŒ€ì²´)
            return_colbert_vecs=True, # âœ… ColBERT ë‹¤ì¤‘ ë²¡í„° (ì •ë°€ ë§¤ì¹­)
            max_length=1024,          # âœ… ê³¼í•™ ë¬¸ì„œì— ìµœì  (ê¸°ì¡´ 512â†’1024)
            batch_size=12
        )

        # ê²°ê³¼ ì €ì¥ (ì„¸ ê°€ì§€ ìœ í˜• ëª¨ë‘)
        dense_vecs = embeddings['dense_vecs']
        lexical_weights = embeddings['lexical_weights']
        colbert_vecs = embeddings['colbert_vecs']

        for j, docid in enumerate(batch_docids):
            embeddings_dict[docid] = {
                'dense': dense_vecs[j],        # numpy array (1024,)
                'sparse': lexical_weights[j],  # dict {token_id: weight}
                'colbert': colbert_vecs[j]     # numpy array (N, 1024)
            }

    except Exception as e:
        print(f"\nâš ï¸  ë°°ì¹˜ {i//batch_size} ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ê°œë³„ ì²˜ë¦¬
        for docid, content in zip(batch_docids, batch_contents):
            try:
                embedding = model.encode(
                    [content],
                    return_dense=True,
                    return_sparse=True,
                    return_colbert_vecs=True,
                    max_length=1024
                )
                embeddings_dict[docid] = {
                    'dense': embedding['dense_vecs'][0],
                    'sparse': embedding['lexical_weights'][0],
                    'colbert': embedding['colbert_vecs'][0]
                }
            except Exception as e2:
                print(f"âš ï¸  ë¬¸ì„œ {docid} ì‹¤íŒ¨: {e2}")
                # ë¹ˆ ë²¡í„°ë¡œ ëŒ€ì²´
                embeddings_dict[docid] = {
                    'dense': np.zeros(1024, dtype=np.float32),
                    'sparse': {},
                    'colbert': np.zeros((1, 1024), dtype=np.float32)
                }

print(f"\nâœ… {len(embeddings_dict)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

# ì €ì¥
output_path = 'embeddings_test_bgem3_optimized.pkl'
print(f"\nì„ë² ë”© ì €ì¥ ì¤‘: {output_path}")
with open(output_path, 'wb') as f:
    pickle.dump(embeddings_dict, f)

print(f"âœ… ì €ì¥ ì™„ë£Œ!")

# í†µê³„ ì¶œë ¥
sample_docid = list(embeddings_dict.keys())[0]
sample_emb = embeddings_dict[sample_docid]

print(f"\nğŸ“Š ì„ë² ë”© í†µê³„:")
print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {len(embeddings_dict)}")
print(f"  - Dense ë²¡í„° ì°¨ì›: {len(sample_emb['dense'])}")
print(f"  - Sparse í† í° ìˆ˜ (ìƒ˜í”Œ): {len(sample_emb['sparse'])}")
print(f"  - ColBERT ë²¡í„° ìˆ˜ (ìƒ˜í”Œ): {sample_emb['colbert'].shape[0]}")
print(f"  - ColBERT ë²¡í„° ì°¨ì›: {sample_emb['colbert'].shape[1]}")
print(f"  - ë°ì´í„° íƒ€ì…: {sample_emb['dense'].dtype}")
print(f"  - íŒŒì¼ í¬ê¸°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")

print("\n" + "="*80)
print("âœ… BGE-M3 ìµœì í™” ì„ë² ë”© ìƒì„± ì™„ë£Œ")
print("="*80)
print("ë‹¤ìŒ ë‹¨ê³„: hybrid_bgem3_optimized.py êµ¬í˜„")
print("  - Dense + Sparse + ColBERT í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§")
print("  - ê°€ì¤‘ì¹˜ ì¡°í•©: w1Â·s_dense + w2Â·s_lex + w3Â·s_mul")
print("="*80)
