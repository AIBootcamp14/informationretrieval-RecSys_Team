"""
RAG Phase 2 ê°œì„  ë²„ì „
ëª©í‘œ: MAP 0.65 -> 0.80+ ë‹¬ì„±
ì£¼ìš” ê°œì„ ì‚¬í•­:
1. Query Rewrite ì‹œìŠ¤í…œ (ì¶•ì•½ì–´ í™•ì¥, ì˜¤íƒ€ êµì •, ë™ì˜ì–´ í™•ì¥)
2. ë©€í‹°í„´ ëŒ€í™” ìµœì í™” (standalone query ê°œì„ )
3. Hybrid Search ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
"""

import os
import json
import re
from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from openai import OpenAI
import traceback

# Load environment variables
load_dotenv()

# ============================================
# Phase 1 ê°œì„ ì‚¬í•­ í¬í•¨
# ============================================

# ì¼ë°˜ ëŒ€í™”ë¡œ í™•ì¸ëœ eval_id ë¦¬ìŠ¤íŠ¸
NORMAL_CHAT_IDS = [276, 261, 233, 90, 222, 37, 70, 153, 169, 235, 91, 265, 141, 26, 183, 260, 51, 30, 165, 60]

# ì¼ë°˜ ëŒ€í™” í‚¤ì›Œë“œ
SMALLTALK_KEYWORDS = [
    'ì•ˆë…•', 'ë°˜ê°€', 'ë°˜ê°‘', 'í•˜ì´', 'hi', 'hello', 'bye', 'ì˜ê°€',
    'í˜ë“¤', 'ì‹ ë‚˜', 'ë¬´ì„œì›Œ', 'ë¬´ì„­', 'ê´œì°®', 'ì¢‹ì•„', 'ì‹«ì–´', 'ìŠ¬í¼', 'ê¸°ë»',
    'ê³ ë§ˆì›Œ', 'ê°ì‚¬', 'ì˜í•´ì¤˜ì„œ', 'ë˜‘ë˜‘', 'ì˜í•˜ëŠ”', 'ëŒ€ë‹¨',
    'ì–´ë•Œ', 'ë­ì•¼', 'ë­í•´', 'ì–´ë–»ê²Œ', 'ì™œ',
    'ë‚¨ë…€ ê´€ê³„', 'ê²°í˜¼', 'ì—°ì• ', 'ì‚¬ë‘'
]

# ê³¼í•™ ê´€ë ¨ í‚¤ì›Œë“œ
SCIENCE_KEYWORDS = [
    'DNA', 'RNA', 'ì„¸í¬', 'ì›ì', 'ë¶„ì', 'í™”í•™', 'ë¬¼ë¦¬', 'ìƒë¬¼', 'ì§„í™”', 'ìœ ì „',
    'ê´‘í•©ì„±', 'ì—ë„ˆì§€', 'ì „ì', 'ì¤‘ë ¥', 'ìê¸°ì¥', 'ì˜¨ë„', 'ì••ë ¥', 'ì†ë„', 'ì§ˆëŸ‰',
    'ë°•í…Œë¦¬ì•„', 'ë°”ì´ëŸ¬ìŠ¤', 'ë‹¨ë°±ì§ˆ', 'íš¨ì†Œ', 'í˜¸ë¥´ëª¬', 'ì‹ ê²½', 'ë‡Œ', 'í˜ˆì•¡',
    'ì‚°ì†Œ', 'ìˆ˜ì†Œ', 'íƒ„ì†Œ', 'ì§ˆì†Œ', 'ì›ì†Œ', 'í™”í•©ë¬¼', 'ë°˜ì‘', 'ì—°ì†Œ', 'ì‚°í™”',
    'í–‰ì„±', 'íƒœì–‘', 'ë‹¬', 'ë³„', 'ì€í•˜', 'ìš°ì£¼', 'ë¸”ë™í™€', 'ë¹…ë±…', 'ìƒëŒ€ì„±',
    'ì „ë¥˜', 'ì „ì••', 'ì €í•­', 'ìê¸°', 'ì „ê¸°', 'íšŒë¡œ', 'ë°˜ë„ì²´', 'íŒŒë™', 'ì£¼íŒŒìˆ˜'
]

# ============================================
# Phase 2 ê°œì„  1: Query Rewrite ì‹œìŠ¤í…œ
# ============================================

# ê³¼í•™ ìš©ì–´ ì¶•ì•½ì–´ ì‚¬ì „
ABBREVIATION_DICT = {
    # í•œê¸€ ì¶•ì•½ì–´ -> ì •ì‹ ëª…ì¹­
    'ë””ì—”ì—ì´': 'DNA',
    'ì•„ë¥´ì—”ì—ì´': 'RNA',
    'ì— ì•Œì—”ì—ì´': 'mRNA',
    'ì—ì´í‹°í”¼': 'ATP',
    'í”¼ì—ì´ì¹˜': 'pH',
    # ì˜ì–´ ì¶•ì•½ì–´ -> í™•ì¥
    'DNA': 'DNA ë””ì˜¥ì‹œë¦¬ë³´í•µì‚° ìœ ì „ì',
    'RNA': 'RNA ë¦¬ë³´í•µì‚°',
    'ATP': 'ATP ì•„ë°ë…¸ì‹ ì‚¼ì¸ì‚° ì—ë„ˆì§€',
    'pH': 'pH ìˆ˜ì†Œì´ì˜¨ë†ë„',
    'CO2': 'CO2 ì´ì‚°í™”íƒ„ì†Œ',
    'O2': 'O2 ì‚°ì†Œ',
    'H2O': 'H2O ë¬¼ water',
}

# ë™ì˜ì–´ ì‚¬ì „
SYNONYM_DICT = {
    'ê´‘í•©ì„±': ['photosynthesis', 'ë¹›í•©ì„±', 'ì—½ë¡ì²´'],
    'ì„¸í¬': ['cell', 'ì„¸í¬ì§ˆ', 'ì„¸í¬ë§‰', 'ì„¸í¬ë²½'],
    'ì§„í™”': ['evolution', 'ìì—°ì„ íƒ', 'ì ì‘', 'ë³€ì´'],
    'ìœ ì „': ['heredity', 'ìœ ì „ì', 'ì—¼ìƒ‰ì²´', 'gene'],
    'ë°”ì´ëŸ¬ìŠ¤': ['virus', 'ë³‘ì›ì²´', 'ê°ì—¼ì²´'],
    'ë°•í…Œë¦¬ì•„': ['bacteria', 'ì„¸ê· ', 'ë¯¸ìƒë¬¼'],
    'ì›ì': ['atom', 'ì›ìí•µ', 'ì „ì'],
    'ë¶„ì': ['molecule', 'í™”í•©ë¬¼', 'ê²°í•©'],
    'ì—ë„ˆì§€': ['energy', 'ì—´', 'ì¼', 'í˜'],
    'ì „ê¸°': ['electricity', 'ì „ë¥˜', 'ì „ì••', 'ì „ì'],
    'ìê¸°': ['magnetism', 'ìê¸°ì¥', 'ìì„'],
    'ì¤‘ë ¥': ['gravity', 'ë§Œìœ ì¸ë ¥', 'ì¤‘ë ¥ì¥'],
    'ë¹›': ['light', 'ê´‘ì„ ', 'ì „ìê¸°íŒŒ', 'ê´‘ì'],
    'ì—´': ['heat', 'ì˜¨ë„', 'ì—´ì—ë„ˆì§€', 'ì—´ëŸ‰'],
    'ì••ë ¥': ['pressure', 'ê¸°ì••', 'ìˆ˜ì••'],
    'ì†ë„': ['velocity', 'speed', 'ì†ë ¥'],
    'ê°€ì†ë„': ['acceleration', 'ê°€ì†', 'ê°ì†'],
}

# ì˜¤íƒ€ êµì • ì‚¬ì „
TYPO_CORRECTIONS = {
    'ê´‘í•©ì„±': ['ê´Œí•©ì„±', 'ê´‘í•™ì„±', 'ê´‘í•©ì…©'],
    'ì„¸í¬': ['ìƒˆí¬', 'ì„¸í‘œ', 'ì„¸í¬'],
    'ì—¼ìƒ‰ì²´': ['ì—½ìƒ‰ì²´', 'ì—¼ìƒ‰ì±„', 'ì—„ìƒ‰ì²´'],
    'ë¶„ì': ['ë¶„ì¬', 'ë¶„ì', 'ë¶„ì§œ'],
    'ì›ì': ['ì›ì¬', 'ì›ì§œ', 'ì›ì'],
    'ì§„í™”': ['ì§„ì™€', 'ì§„í™”', 'ì§„í•˜'],
    'ìœ ì „': ['ìœ ì „', 'ìœ ì ¼', 'ìœ ì •'],
}

def expand_abbreviations(query):
    """ì¶•ì•½ì–´ë¥¼ í™•ì¥"""
    expanded = query
    for abbr, expansion in ABBREVIATION_DICT.items():
        if abbr in expanded:
            # ì¶•ì•½ì–´ë¥¼ í™•ì¥ëœ í˜•íƒœë¡œ êµì²´
            expanded = expanded.replace(abbr, expansion)
    return expanded

def add_synonyms(query):
    """ë™ì˜ì–´ ì¶”ê°€"""
    words = query.split()
    additional_terms = []

    for word in words:
        for key, synonyms in SYNONYM_DICT.items():
            if key in word:
                # ë™ì˜ì–´ ì¤‘ 1-2ê°œë§Œ ì¶”ê°€
                additional_terms.extend(synonyms[:2])
                break

    if additional_terms:
        return f"{query} {' '.join(additional_terms)}"
    return query

def correct_typos(query):
    """ì˜¤íƒ€ êµì •"""
    corrected = query
    for correct, typos in TYPO_CORRECTIONS.items():
        for typo in typos:
            if typo in corrected and typo != correct:
                corrected = corrected.replace(typo, correct)
    return corrected

def rewrite_query(query, conversation_history=None):
    """
    Query Rewrite ì¢…í•© í•¨ìˆ˜

    Args:
        query: ì›ë³¸ ì¿¼ë¦¬
        conversation_history: ëŒ€í™” ì´ë ¥ (ë©€í‹°í„´ì¸ ê²½ìš°)

    Returns:
        str: ê°œì„ ëœ ì¿¼ë¦¬
    """
    # Step 1: ì˜¤íƒ€ êµì •
    rewritten = correct_typos(query)

    # Step 2: ì¶•ì•½ì–´ í™•ì¥
    rewritten = expand_abbreviations(rewritten)

    # Step 3: ë™ì˜ì–´ ì¶”ê°€
    rewritten = add_synonyms(rewritten)

    # Step 4: ë¶ˆí•„ìš”í•œ ì¡°ì‚¬/ì–´ë¯¸ ì œê±°
    # í•œêµ­ì–´ ì¡°ì‚¬ ì œê±° íŒ¨í„´
    particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ì´ë‚˜', 'ë‚˜']
    for particle in particles:
        rewritten = re.sub(f' {particle} ', ' ', rewritten)

    # Step 5: ì¤‘ë³µ ì œê±°
    words = rewritten.split()
    unique_words = []
    seen = set()
    for word in words:
        if word.lower() not in seen:
            unique_words.append(word)
            seen.add(word.lower())

    return ' '.join(unique_words)

# ============================================
# Phase 2 ê°œì„  2: ë©€í‹°í„´ ëŒ€í™” ìµœì í™”
# ============================================

def create_standalone_query(messages, client, llm_model="solar-pro2"):
    """
    ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ë…ë¦½ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±

    Args:
        messages: ëŒ€í™” ì´ë ¥
        client: OpenAI client
        llm_model: LLM ëª¨ë¸

    Returns:
        str: standalone query
    """
    if not messages or len(messages) == 1:
        return messages[-1]['content'] if messages else ""

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = []
    for msg in messages[:-1]:
        role = msg.get('role', 'user')
        content = msg.get('content', '')
        context.append(f"{role}: {content}")

    context_str = "\n".join(context)
    current_query = messages[-1]['content']

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í˜„ì¬ ì§ˆë¬¸ì„ ë…ë¦½ì ì¸ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”.

ëŒ€í™” ë§¥ë½:
{context_str}

í˜„ì¬ ì§ˆë¬¸: {current_query}

ê·œì¹™:
1. ëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ, ê±°ê¸° ë“±)ë¥¼ êµ¬ì²´ì  ëª…ì‚¬ë¡œ ì¹˜í™˜
2. ì´ì „ ëŒ€í™”ì˜ ì£¼ì œë¥¼ ëª…í™•íˆ í¬í•¨
3. ê²€ìƒ‰ì— ìµœì í™”ëœ í‚¤ì›Œë“œ ì¤‘ì‹¬ ì¿¼ë¦¬
4. ì¿¼ë¦¬ë§Œ ë°˜í™˜ (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´)

ë…ë¦½ ì¿¼ë¦¬:"""

    try:
        response = client.chat.completions.create(
            model=llm_model,
            messages=[
                {"role": "system", "content": "ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€"},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=100
        )

        standalone = response.choices[0].message.content.strip()

        # Query rewrite ì ìš©
        standalone = rewrite_query(standalone)

        return standalone

    except Exception as e:
        print(f"Standalone query ìƒì„± ì‹¤íŒ¨: {e}")
        return current_query

# ============================================
# Phase 2 ê°œì„  3: Hybrid Search ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
# ============================================

def calculate_query_characteristics(query):
    """
    ì¿¼ë¦¬ íŠ¹ì„± ë¶„ì„

    Returns:
        dict: ì¿¼ë¦¬ íŠ¹ì„± ì ìˆ˜
    """
    characteristics = {
        'science_density': 0,  # ê³¼í•™ ìš©ì–´ ë°€ë„
        'conceptual': 0,       # ê°œë… ì„¤ëª… ìš”êµ¬ë„
        'specific': 0,         # êµ¬ì²´ì„±
        'length': len(query.split())
    }

    # ê³¼í•™ ìš©ì–´ ë°€ë„ ê³„ì‚°
    words = query.lower().split()
    science_count = 0
    for word in words:
        for keyword in SCIENCE_KEYWORDS:
            if keyword.lower() in word:
                science_count += 1
                break

    characteristics['science_density'] = science_count / max(len(words), 1)

    # ê°œë… ì„¤ëª… ìš”êµ¬ë„ (ì„¤ëª…, ì•Œë ¤ì¤˜, ë¬´ì—‡ì¸ê°€ ë“±)
    conceptual_patterns = ['ì„¤ëª…', 'ì•Œë ¤', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì›ë¦¬', 'ê³¼ì •', 'ë°©ë²•', 'ì´ìœ ']
    for pattern in conceptual_patterns:
        if pattern in query:
            characteristics['conceptual'] += 1

    characteristics['conceptual'] = min(characteristics['conceptual'] / 3, 1.0)

    # êµ¬ì²´ì„± (íŠ¹ì • ìš©ì–´, ìˆ«ì, ê³ ìœ ëª…ì‚¬ ë“±)
    specific_patterns = [r'\d+', r'[A-Z]{2,}', r'DNA', r'RNA', r'ATP']
    for pattern in specific_patterns:
        if re.search(pattern, query):
            characteristics['specific'] += 1

    characteristics['specific'] = min(characteristics['specific'] / 3, 1.0)

    return characteristics

def get_dynamic_weights(query):
    """
    ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°

    Returns:
        dict: BM25ì™€ Dense ê°€ì¤‘ì¹˜
    """
    chars = calculate_query_characteristics(query)

    # ê¸°ë³¸ ê°€ì¤‘ì¹˜
    weights = {'bm25': 0.5, 'dense': 0.5}

    # ê³¼í•™ ìš©ì–´ê°€ ë§ìœ¼ë©´ BM25 ê°€ì¤‘ì¹˜ ì¦ê°€
    if chars['science_density'] > 0.3:
        weights['bm25'] = 0.7
        weights['dense'] = 0.3

    # ê°œë… ì„¤ëª…ì´ í•„ìš”í•˜ë©´ Dense ê°€ì¤‘ì¹˜ ì¦ê°€
    elif chars['conceptual'] > 0.5:
        weights['bm25'] = 0.3
        weights['dense'] = 0.7

    # ë§¤ìš° êµ¬ì²´ì ì¸ ì¿¼ë¦¬ë©´ BM25 ìš°ì„ 
    elif chars['specific'] > 0.5:
        weights['bm25'] = 0.8
        weights['dense'] = 0.2

    # ì§§ì€ ì¿¼ë¦¬ë©´ Dense í™œìš©
    elif chars['length'] < 3:
        weights['bm25'] = 0.4
        weights['dense'] = 0.6

    return weights

def adaptive_hybrid_search(es, query, model, size=3):
    """
    ì ì‘í˜• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

    Args:
        es: Elasticsearch client
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        model: Sentence Transformer
        size: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜

    Returns:
        list: ê²€ìƒ‰ ê²°ê³¼
    """
    # Query rewrite ì ìš©
    rewritten_query = rewrite_query(query)

    # ë™ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
    weights = get_dynamic_weights(rewritten_query)

    print(f"Query: {query[:50]}...")
    print(f"Rewritten: {rewritten_query[:50]}...")
    print(f"Weights: BM25={weights['bm25']:.2f}, Dense={weights['dense']:.2f}")

    # BM25 ê²€ìƒ‰
    bm25_query = {
        "match": {
            "content": {
                "query": rewritten_query
            }
        }
    }
    bm25_results = es.search(index="test", query=bm25_query, size=size*3)

    # Dense ê²€ìƒ‰
    query_embedding = model.encode([rewritten_query])[0]
    knn = {
        "field": "embeddings",
        "query_vector": query_embedding.tolist(),
        "k": size*3,
        "num_candidates": 100
    }
    dense_results = es.search(index="test", knn=knn)

    # ê²°ê³¼ ë³‘í•© (ê°€ì¤‘ì¹˜ ì ìš©)
    doc_scores = {}

    # BM25 ê²°ê³¼ ì²˜ë¦¬
    for rank, hit in enumerate(bm25_results['hits']['hits']):
        docid = hit['_source'].get('docid', '')
        if docid:
            score = hit['_score'] * weights['bm25']
            doc_scores[docid] = {
                'score': score,
                'source': hit['_source'],
                'bm25_rank': rank + 1
            }

    # Dense ê²°ê³¼ ì²˜ë¦¬ ë° ë³‘í•©
    for rank, hit in enumerate(dense_results['hits']['hits']):
        docid = hit['_source'].get('docid', '')
        if docid:
            dense_score = (1 / (1 + hit['_score'])) * weights['dense']  # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜

            if docid in doc_scores:
                doc_scores[docid]['score'] += dense_score
                doc_scores[docid]['dense_rank'] = rank + 1
            else:
                doc_scores[docid] = {
                    'score': dense_score,
                    'source': hit['_source'],
                    'dense_rank': rank + 1
                }

    # ì ìˆ˜ìˆœ ì •ë ¬
    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1]['score'], reverse=True)

    # ìƒìœ„ Nê°œ ë°˜í™˜
    results = []
    for docid, data in sorted_docs[:size]:
        results.append({
            'docid': docid,
            'score': data['score'],
            'content': data['source'].get('content', ''),
            'ranks': {
                'bm25': data.get('bm25_rank', -1),
                'dense': data.get('dense_rank', -1)
            }
        })

    return results

# ============================================
# Phase 1 ê°œì„ ì‚¬í•­ ìœ ì§€
# ============================================

def is_smalltalk(query, eval_id=None):
    """ì¼ë°˜ ëŒ€í™” íŒë‹¨ (Phase 1ì—ì„œ ê°€ì ¸ì˜´)"""
    if eval_id and eval_id in NORMAL_CHAT_IDS:
        return True

    query_lower = query.lower()
    for keyword in SCIENCE_KEYWORDS:
        if keyword.lower() in query_lower:
            return False

    for keyword in SMALLTALK_KEYWORDS:
        if keyword in query:
            if len(query) < 30:
                return True

    if len(query) < 10:
        return True

    question_words = ['ë­ì•¼', 'ë­í•´', 'ì–´ë•Œ', 'ì–´ë–»ê²Œ', 'ì™œ']
    for word in question_words:
        if query.strip() == word or query.strip() == word + '?':
            return True

    return False

def get_dynamic_topk_v2(results, base_threshold=5):
    """
    Phase 2 ê°œì„ ëœ ë™ì  TopK
    ë” ì„¸ë°€í•œ threshold ì¡°ì •
    """
    if not results:
        return []

    selected = []

    # ì ìˆ˜ ë¶„í¬ ë¶„ì„
    scores = [r['score'] for r in results]
    max_score = max(scores) if scores else 0

    # ë” ì„¸ë°€í•œ threshold
    if max_score < base_threshold * 0.5:  # ë§¤ìš° ë‚®ìŒ
        return []
    elif max_score < base_threshold:  # ë‚®ìŒ
        threshold = base_threshold * 0.5
        max_docs = 1
    elif max_score < base_threshold * 2:  # ì¤‘ê°„
        threshold = base_threshold * 0.7
        max_docs = 2
    else:  # ë†’ìŒ
        threshold = base_threshold
        max_docs = 3

    # Threshold ì´ìƒì¸ ë¬¸ì„œë§Œ ì„ íƒ
    for doc in results[:max_docs]:
        if doc['score'] >= threshold:
            selected.append(doc)

    return selected

# ============================================
# í†µí•© RAG íŒŒì´í”„ë¼ì¸
# ============================================

def phase2_answer_question(messages, es, model, client, llm_model="solar-pro2", eval_id=None):
    """
    Phase 2 ê°œì„ ì´ ëª¨ë‘ ì ìš©ëœ RAG íŒŒì´í”„ë¼ì¸
    """
    response = {
        "eval_id": eval_id,
        "standalone_query": "",
        "topk": [],
        "references": [],
        "answer": ""
    }

    # ë©”ì‹œì§€ì—ì„œ ì¿¼ë¦¬ ì¶”ì¶œ
    if isinstance(messages, list) and len(messages) > 0:
        current_query = messages[-1].get('content', '') if isinstance(messages[-1], dict) else str(messages[-1])
    else:
        current_query = str(messages)

    # ========================================
    # STEP 1: ì¼ë°˜ ëŒ€í™” í•„í„°ë§ (Phase 1)
    # ========================================
    if is_smalltalk(current_query, eval_id):
        print(f"[Smalltalk] ID: {eval_id}, Query: {current_query[:50]}")
        response["standalone_query"] = current_query
        response["topk"] = []
        response["references"] = []

        try:
            chat_result = client.chat.completions.create(
                model=llm_model,
                messages=[
                    {"role": "system", "content": "ì¹œê·¼í•œ ëŒ€í™” ìƒëŒ€ë¡œì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": current_query}
                ],
                temperature=0.7,
                max_tokens=200
            )
            response["answer"] = chat_result.choices[0].message.content
        except:
            response["answer"] = "ë„¤, ë§ìŠµë‹ˆë‹¤."

        return response

    # ========================================
    # STEP 2: Standalone Query ìƒì„± (Phase 2)
    # ========================================
    if len(messages) > 1:
        standalone_query = create_standalone_query(messages, client, llm_model)
    else:
        standalone_query = rewrite_query(current_query)

    response["standalone_query"] = standalone_query

    # ========================================
    # STEP 3: ì ì‘í˜• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Phase 2)
    # ========================================
    search_results = adaptive_hybrid_search(es, standalone_query, model, size=10)

    # ========================================
    # STEP 4: ë™ì  TopK ì ìš© (Phase 1 + 2)
    # ========================================
    selected_docs = get_dynamic_topk_v2(search_results, base_threshold=5)

    # ê²°ê³¼ ì •ë¦¬
    topk_ids = []
    references = []

    for doc in selected_docs:
        topk_ids.append(doc['docid'])
        references.append({
            "docid": doc['docid'],
            "score": doc['score'],
            "content": doc['content'][:500]
        })

    response["topk"] = topk_ids
    response["references"] = references

    # ========================================
    # STEP 5: ë‹µë³€ ìƒì„±
    # ========================================
    if references:
        context = "\n\n".join([f"[ë¬¸ì„œ {i+1}]\n{ref['content']}" for i, ref in enumerate(references)])

        qa_prompt = f"""ë‹¤ìŒ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {current_query}

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì •í™•íˆ í™œìš©
2. êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì„¤ëª…
3. ê³¼í•™ì  ìš©ì–´ëŠ” ì •í™•í•˜ê²Œ ì‚¬ìš©
4. í•„ìš”ì‹œ ì˜ˆì‹œ í¬í•¨

ë‹µë³€:"""

        try:
            result = client.chat.completions.create(
                model=llm_model,
                messages=[
                    {"role": "system", "content": "ê³¼í•™ ìƒì‹ ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”."},
                    {"role": "user", "content": qa_prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )
            response["answer"] = result.choices[0].message.content
        except Exception as e:
            response["answer"] = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    else:
        response["answer"] = "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return response

# ============================================
# í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜
# ============================================

def run_phase2_evaluation():
    """Phase 2 ê°œì„  ì‚¬í•­ì„ ì ìš©í•˜ì—¬ í‰ê°€ ì‹¤í–‰"""

    print("=" * 50)
    print("Phase 2 ê°œì„  í‰ê°€ ì‹œì‘")
    print("ê°œì„ ì‚¬í•­: Query Rewrite, ë©€í‹°í„´ ìµœì í™”, Hybrid ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •")
    print("=" * 50)

    # Elasticsearch ì—°ê²°
    es_username = "elastic"
    es_password = os.getenv("ELASTICSEARCH_PASSWORD")

    es = Elasticsearch(
        ['http://localhost:9200'],
        basic_auth=(es_username, es_password),
        verify_certs=False
    )

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")

    # Upstage client ì´ˆê¸°í™”
    upstage_api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(
        base_url="https://api.upstage.ai/v1/solar",
        api_key=upstage_api_key
    )

    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    eval_data = []
    with open("../data/eval.jsonl", "r", encoding="utf-8") as f:
        for line in f:
            eval_data.append(json.loads(line))

    print(f"ì´ í‰ê°€ ë°ì´í„°: {len(eval_data)}ê°œ\n")

    # ê²°ê³¼ ì €ì¥
    results = []

    # í†µê³„
    stats = {
        'smalltalk': 0,
        'no_results': 0,
        'topk_dist': {0: 0, 1: 0, 2: 0, 3: 0}
    }

    # ê° í‰ê°€ í•­ëª© ì²˜ë¦¬
    for idx, item in enumerate(eval_data):
        eval_id = item['eval_id']
        messages = item['msg']

        print(f"\n[{idx+1}/{len(eval_data)}] Processing eval_id: {eval_id}")

        try:
            # Phase 2 ê°œì„  RAG ì‹¤í–‰
            result = phase2_answer_question(
                messages=messages,
                es=es,
                model=model,
                client=client,
                eval_id=eval_id
            )

            results.append(result)

            # í†µê³„ ì—…ë°ì´íŠ¸
            topk_count = len(result['topk'])
            stats['topk_dist'][min(topk_count, 3)] += 1

            if topk_count == 0:
                if is_smalltalk(messages[-1]['content'] if messages else "", eval_id):
                    stats['smalltalk'] += 1
                    print(f"  -> ì¼ë°˜ ëŒ€í™”ë¡œ ë¶„ë¥˜")
                else:
                    stats['no_results'] += 1
                    print(f"  -> ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
            else:
                print(f"  -> {topk_count}ê°œ ë¬¸ì„œ ê²€ìƒ‰ (Query: {result['standalone_query'][:40]}...)")

        except Exception as e:
            print(f"  -> ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            traceback.print_exc()
            results.append({
                "eval_id": eval_id,
                "standalone_query": messages[-1]['content'] if messages else "",
                "topk": [],
                "references": [],
                "answer": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            })

    # ê²°ê³¼ ì €ì¥
    output_file = "phase2_submission.csv"
    with open(output_file, "w", encoding="utf-8") as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + "\n")

    print(f"\n" + "=" * 50)
    print(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    print("=" * 50)

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š í†µê³„:")
    print(f"  - ì¼ë°˜ ëŒ€í™”: {stats['smalltalk']}ê°œ")
    print(f"  - ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ: {stats['no_results']}ê°œ")
    print(f"  - TopK ë¶„í¬: {stats['topk_dist']}")

    # Phase 1 ëŒ€ë¹„ ê°œì„  ë¶„ì„
    print("\nğŸ“ˆ ì˜ˆìƒ ê°œì„  íš¨ê³¼:")
    print("  - Query Rewrite: ì¶•ì•½ì–´/ì˜¤íƒ€ êµì •ìœ¼ë¡œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ")
    print("  - ë©€í‹°í„´ ìµœì í™”: 20ê°œ ë©€í‹°í„´ ëŒ€í™” ì²˜ë¦¬ ê°œì„ ")
    print("  - Hybrid ê°€ì¤‘ì¹˜: ì¿¼ë¦¬ íŠ¹ì„±ë³„ ìµœì  ê²€ìƒ‰ ì „ëµ ì ìš©")
    print("\nëª©í‘œ MAP: 0.75~0.80")

if __name__ == "__main__":
    run_phase2_evaluation()