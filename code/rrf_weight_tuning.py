"""
RRF Weight Tuning: query_expansion_v1 ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™”
ëª©í‘œ: w_bm25, w_bgem3 ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ 0.9+ MAP@3 ë‹¬ì„±

ì‹¤í—˜:
1. w_bm25=0.4, w_bgem3=0.6 (BM25 ì¦ê°€)
2. w_bm25=0.2, w_bgem3=0.8 (BGE-M3 ì¦ê°€)
3. w_bm25=0.25, w_bgem3=0.75 (ì•½ê°„ BGE-M3 ê°•ì¡°)
"""

import json
import os
import pickle
import numpy as np
from tqdm import tqdm
from elasticsearch import Elasticsearch
from FlagEmbedding import BGEM3FlagModel
from openai import OpenAI

# ES ì—°ê²°
es = Elasticsearch(['http://localhost:9200'])

# Solar API ì´ˆê¸°í™”
upstage_api_key = os.environ.get('UPSTAGE_API_KEY')
client = None
if upstage_api_key:
    client = OpenAI(
        api_key=upstage_api_key,
        base_url="https://api.upstage.ai/v1/solar"
    )

# BGE-M3 ëª¨ë¸ ë¡œë“œ
print("BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
print("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ì¼ë°˜ ëŒ€í™” ID
SMALLTALK_IDS = {276, 261, 233, 90, 222, 235, 165, 153, 169, 141, 183}

# BGE-M3 ìµœì í™” ì„ë² ë”© ë¡œë“œ
print("\nBGE-M3 ìµœì í™” ì„ë² ë”© ë¡œë“œ ì¤‘...")
with open('embeddings_test_bgem3_optimized.pkl', 'rb') as f:
    embeddings_dict = pickle.load(f)
print(f"âœ… {len(embeddings_dict)}ê°œ ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")

def rewrite_query_with_context(msg):
    """ë©€í‹°í„´ ëŒ€í™”ì˜ ë§¥ë½ì„ í†µí•©í•˜ì—¬ ì¿¼ë¦¬ ì¬ì‘ì„±"""
    if isinstance(msg, str):
        return msg

    if len(msg) == 1:
        return msg[0]['content']

    current_query = msg[-1]['content']

    # ëŒ€ëª…ì‚¬ë‚˜ ëª¨í˜¸í•œ í‘œí˜„ í™•ì¸
    ambiguous_terms = ['ê·¸ ', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì´ê±°', 'ì €ê²ƒ', 'ì €ê±°', 'ì™œ', 'ì–´ë–»ê²Œ', 'ì´ìœ ']

    if not any(term in current_query for term in ambiguous_terms):
        return current_query

    if not client:
        return current_query

    # LLMìœ¼ë¡œ ì¿¼ë¦¬ ì¬ì‘ì„±
    conversation_context = "\n".join([
        f"{m['role']}: {m['content']}" for m in msg[:-1]
    ])

    prompt = f"""ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_context}

í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{current_query}"

ì´ ì§ˆë¬¸ì„ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
ëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ ë“±)ë¥¼ êµ¬ì²´ì ì¸ ëª…ì‚¬ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.

ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="solar-pro",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=150
        )

        rewritten = response.choices[0].message.content.strip()
        rewritten = rewritten.strip('"').strip("'")

        return rewritten

    except Exception as e:
        return current_query

def search_bm25(query, top_k=20):
    """BM25 ê²€ìƒ‰"""
    fetch_size = top_k + 5

    response = es.search(
        index='test',
        body={
            'query': {
                'match': {
                    'content': {
                        'query': query,
                        'analyzer': 'nori'
                    }
                }
            },
            'size': fetch_size
        }
    )

    if not response['hits']['hits']:
        return []

    # original_docid ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    seen_original_docids = set()
    results = []

    for hit in response['hits']['hits']:
        source = hit['_source']
        original_docid = source.get('original_docid', source['docid'])

        if original_docid in seen_original_docids:
            continue

        seen_original_docids.add(original_docid)
        results.append({
            'docid': original_docid,
            'content': source['content'],
            'score': hit['_score'],
            'source': 'bm25'
        })

        if len(results) >= top_k:
            break

    return results

def bgem3_hybrid_score(query_dense, query_sparse, query_colbert,
                       doc_dense, doc_sparse, doc_colbert,
                       w1=0.4, w2=0.3, w3=0.3):
    """BGE-M3 Hybrid Scoring"""
    # 1. Dense ìœ ì‚¬ë„
    s_dense = np.dot(query_dense, doc_dense) / (
        np.linalg.norm(query_dense) * np.linalg.norm(doc_dense)
    )

    # 2. Sparse ìœ ì‚¬ë„
    s_lex = 0.0
    if query_sparse and doc_sparse:
        common_tokens = set(query_sparse.keys()) & set(doc_sparse.keys())
        for token in common_tokens:
            s_lex += query_sparse[token] * doc_sparse[token]

    # 3. ColBERT ìœ ì‚¬ë„
    s_mul = 0.0
    if query_colbert.shape[0] > 0 and doc_colbert.shape[0] > 0:
        query_colbert_norm = query_colbert / np.linalg.norm(query_colbert, axis=1, keepdims=True)
        doc_colbert_norm = doc_colbert / np.linalg.norm(doc_colbert, axis=1, keepdims=True)
        sim_matrix = np.dot(query_colbert_norm, doc_colbert_norm.T)
        s_mul = np.mean(np.max(sim_matrix, axis=1))

    hybrid_score = w1 * s_dense + w2 * s_lex + w3 * s_mul
    return hybrid_score

def search_bgem3_hybrid(query, embeddings_dict, top_k=20, max_length=128):
    """BGE-M3 Hybrid ê²€ìƒ‰"""
    # BGE-M3 ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = model.encode(
        [query],
        return_dense=True,
        return_sparse=True,
        return_colbert_vecs=True,
        max_length=max_length
    )

    query_dense = query_embedding['dense_vecs'][0]
    query_sparse = query_embedding['lexical_weights'][0]
    query_colbert = query_embedding['colbert_vecs'][0]

    # ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ Hybrid Score ê³„ì‚°
    scores = []
    for docid, doc_emb in embeddings_dict.items():
        score = bgem3_hybrid_score(
            query_dense, query_sparse, query_colbert,
            doc_emb['dense'], doc_emb['sparse'], doc_emb['colbert']
        )
        scores.append((docid, score))

    # ì •ë ¬
    scores.sort(key=lambda x: x[1], reverse=True)

    # ESì—ì„œ content ê°€ì ¸ì˜¤ê¸°
    results = []
    for docid, score in scores[:top_k]:
        try:
            resp = es.search(
                index='test',
                body={
                    'query': {
                        'bool': {
                            'should': [
                                {'term': {'docid.keyword': docid}},
                                {'term': {'original_docid.keyword': docid}}
                            ]
                        }
                    },
                    'size': 1
                }
            )

            if resp['hits']['hits']:
                source = resp['hits']['hits'][0]['_source']
                results.append({
                    'docid': docid,
                    'content': source['content'],
                    'score': float(score),
                    'source': 'bgem3_hybrid'
                })
        except Exception as e:
            continue

    return results

def hybrid_search_rrf(query, embeddings_dict, top_k=20, k=60,
                      query_max_length=128, w_bm25=0.3, w_bgem3=0.7):
    """
    RRFë¡œ BM25 + BGE-M3 Hybrid ê²°í•©

    Args:
        w_bm25: BM25 ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.3)
        w_bgem3: BGE-M3 ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 0.7)
    """
    # BM25 ê²€ìƒ‰
    bm25_results = search_bm25(query, top_k=top_k)

    # BGE-M3 Hybrid ê²€ìƒ‰
    bgem3_results = search_bgem3_hybrid(query, embeddings_dict, top_k=top_k, max_length=query_max_length)

    # RRF ìŠ¤ì½”ì–´ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì ìš©)
    rrf_scores = {}
    doc_contents = {}

    for rank, doc in enumerate(bm25_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + w_bm25 / (k + rank)
        doc_contents[docid] = doc['content']

    for rank, doc in enumerate(bgem3_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + w_bgem3 / (k + rank)
        doc_contents[docid] = doc['content']

    # ì •ë ¬
    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

    results = []
    for docid, score in sorted_docs:
        results.append({
            'docid': docid,
            'content': doc_contents[docid],
            'score': score
        })

    return results

def llm_rerank(query, docs, top_k=3):
    """Solar-proë¡œ Reranking"""
    if not docs or len(docs) <= top_k or not client:
        return [doc['docid'] for doc in docs[:top_k]]

    try:
        doc_list = []
        for i, doc in enumerate(docs[:15]):
            content_preview = doc['content'][:300]
            if len(doc['content']) > 300:
                content_preview += "..."
            doc_list.append(f"[{i}] {content_preview}")

        docs_text = "\n\n".join(doc_list)

        response = client.chat.completions.create(
            model="solar-pro",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ê³¼í•™ ì§€ì‹ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ relevance íŒë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì •í™•íˆ ì„ íƒí•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"""ì¿¼ë¦¬: {query}

ë¬¸ì„œë“¤:
{docs_text}

ì´ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ {top_k}ê°œì˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.
- ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ”ê°€?
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ê°€?
- ê³¼í•™ì ìœ¼ë¡œ ì •í™•í•œê°€?

ì¶œë ¥: ë²ˆí˜¸ë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„ (ì˜ˆ: 0,2,4)
ì„¤ëª… ì—†ì´ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
                }
            ],
            temperature=0.0,
            max_tokens=30
        )

        result = response.choices[0].message.content.strip()
        indices = [int(x.strip()) for x in result.split(',') if x.strip().isdigit()]

        reranked_docids = []
        for idx in indices[:top_k]:
            if 0 <= idx < len(docs):
                reranked_docids.append(docs[idx]['docid'])

        # ë¶€ì¡±í•˜ë©´ ì›ë˜ ìˆœì„œë¡œ ì±„ìš°ê¸°
        for doc in docs:
            if len(reranked_docids) >= top_k:
                break
            if doc['docid'] not in reranked_docids:
                reranked_docids.append(doc['docid'])

        return reranked_docids[:top_k]

    except Exception as e:
        return [doc['docid'] for doc in docs[:top_k]]

def rrf_weight_strategy(eval_id, msg, embeddings_dict, w_bm25, w_bgem3):
    """RRF Weight Tuning ì „ëµ"""
    # ì¼ë°˜ ëŒ€í™”ëŠ” ë¹ˆ ê²°ê³¼
    if eval_id in SMALLTALK_IDS:
        return []

    # Step 1: ì¿¼ë¦¬ ì¬ì‘ì„±
    rewritten_query = rewrite_query_with_context(msg)

    # Step 2: Hybrid Search (RRF ê°€ì¤‘ì¹˜ ì¡°ì •)
    hybrid_results = hybrid_search_rrf(
        rewritten_query,
        embeddings_dict,
        top_k=20,
        k=60,
        query_max_length=128,
        w_bm25=w_bm25,  # âœ… ê°€ì¤‘ì¹˜ ì¡°ì •
        w_bgem3=w_bgem3  # âœ… ê°€ì¤‘ì¹˜ ì¡°ì •
    )

    if not hybrid_results:
        return []

    # Step 3: LLM Reranking
    final_topk = llm_rerank(rewritten_query, hybrid_results, top_k=3)

    return final_topk

def run_rrf_weight_experiment(w_bm25, w_bgem3, experiment_name):
    """RRF Weight ì‹¤í—˜ ì‹¤í–‰"""
    print("="*80)
    print(f"RRF Weight Tuning Experiment: {experiment_name}")
    print("="*80)
    print(f"w_bm25 = {w_bm25}")
    print(f"w_bgem3 = {w_bgem3}")
    print("="*80)

    # Eval ë°ì´í„° ë¡œë“œ
    with open('../data/eval.jsonl', 'r', encoding='utf-8') as f:
        eval_data = [json.loads(line) for line in f]

    print(f"\nğŸ“‹ ì´ {len(eval_data)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘\n")

    results = []

    for item in tqdm(eval_data, desc=f"RRF w_bm25={w_bm25} w_bgem3={w_bgem3}"):
        eval_id = item['eval_id']
        msg = item['msg']

        # RRF Weight ì „ëµ ì‹¤í–‰
        topk = rrf_weight_strategy(eval_id, msg, embeddings_dict, w_bm25, w_bgem3)

        results.append({
            'eval_id': eval_id,
            'retrieve': topk
        })

    # ì œì¶œ íŒŒì¼ ìƒì„±
    output_path = f'rrf_w{w_bm25}_{w_bgem3}_submission.csv'
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in results:
            json_obj = {
                'eval_id': r['eval_id'],
                'topk': r['retrieve']
            }
            f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')

    print(f"\n{'='*80}")
    print(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {experiment_name}")
    print(f"{'='*80}")
    print(f"ğŸ’¾ ì œì¶œ íŒŒì¼: {output_path}")
    print(f"{'='*80}")

    return results

if __name__ == "__main__":
    print("\n" + "="*80)
    print("RRF Weight Tuning: 3ê°€ì§€ ê°€ì¤‘ì¹˜ ì¡°í•© ì‹¤í—˜")
    print("="*80)
    print("í˜„ì¬ ìµœê³  ì ìˆ˜: query_expansion_v1 = 0.7848")
    print("ëª©í‘œ: 0.9+ MAP@3")
    print("="*80 + "\n")

    # Experiment 1: w_bm25=0.4, w_bgem3=0.6 (BM25 ì¦ê°€)
    print("\n" + "="*80)
    print("Experiment 1: w_bm25=0.4, w_bgem3=0.6 (BM25 ì˜í–¥ë ¥ ì¦ê°€)")
    print("="*80)
    run_rrf_weight_experiment(
        w_bm25=0.4,
        w_bgem3=0.6,
        experiment_name="BM25 Boost"
    )

    # Experiment 2: w_bm25=0.2, w_bgem3=0.8 (BGE-M3 ì¦ê°€)
    print("\n" + "="*80)
    print("Experiment 2: w_bm25=0.2, w_bgem3=0.8 (BGE-M3 ì˜í–¥ë ¥ ì¦ê°€)")
    print("="*80)
    run_rrf_weight_experiment(
        w_bm25=0.2,
        w_bgem3=0.8,
        experiment_name="BGE-M3 Boost"
    )

    # Experiment 3: w_bm25=0.25, w_bgem3=0.75 (ì•½ê°„ BGE-M3 ê°•ì¡°)
    print("\n" + "="*80)
    print("Experiment 3: w_bm25=0.25, w_bgem3=0.75 (BGE-M3 ì•½ê°„ ê°•ì¡°)")
    print("="*80)
    run_rrf_weight_experiment(
        w_bm25=0.25,
        w_bgem3=0.75,
        experiment_name="BGE-M3 Slight Emphasis"
    )

    print("\n" + "="*80)
    print("âœ… ëª¨ë“  RRF Weight Tuning ì‹¤í—˜ ì™„ë£Œ")
    print("="*80)
    print("\nì œì¶œ íŒŒì¼:")
    print("  - rrf_w0.4_0.6_submission.csv")
    print("  - rrf_w0.2_0.8_submission.csv")
    print("  - rrf_w0.25_0.75_submission.csv")
    print("="*80)
