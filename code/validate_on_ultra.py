"""
Ultra Validation Script for Cascaded Reranking v1

Tests cascaded_reranking_v1 system on 3 ultra-high-quality validation samples
generated by Solar Pro 5-Phase Pipeline.

Current Best: query_expansion_v1 with MAP@3 = 0.7848
Goal: Validate if cascaded reranking achieves similar or better performance

3 Validation Samples:
- eval_id 205: í”¼ë¥¼ ë§‘ê²Œ í•˜ê³  ë…¸íë¬¼ì„ ì—†ì• ëŠ” ê¸°ê´€ (ì‹ ìž¥)
- eval_id 43: ë‹¬ì˜ ê°™ì€ ë©´ë§Œ ë³´ì´ëŠ” ì´ìœ  (ì¡°ì„ ê³ ì •)
- eval_id 47: ì›ìžë²ˆí˜¸ì™€ ì†Œë¦½ìžì˜ ê´€ê³„ (ì–‘ì„±ìž)
"""

import json
import os
import sys

# Import cascaded reranking strategy from cascaded_reranking_v1.py
from cascaded_reranking_v1 import (
    cascaded_reranking_strategy,
    embeddings_dict,
    SMALLTALK_IDS
)

def load_ultra_validation(path='ultra_validation_solar.jsonl'):
    """Load 3 ultra-high-quality validation samples"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Validation file not found: {path}")

    validation_data = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:  # Skip empty lines
                validation_data.append(json.loads(line))

    print(f"âœ“ Loaded {len(validation_data)} ultra-high-quality validation samples")
    return validation_data

def calculate_ap_at_3(ground_truth, predicted):
    """
    Calculate Average Precision @ 3

    Args:
        ground_truth: List of 3 relevant docids
        predicted: List of 3 predicted docids

    Returns:
        AP score (0.0 to 1.0)
    """
    if not ground_truth or not predicted:
        return 0.0

    ground_truth_set = set(ground_truth)
    num_hits = 0
    sum_precisions = 0.0

    for i, doc_id in enumerate(predicted[:3], 1):
        if doc_id in ground_truth_set:
            num_hits += 1
            precision_at_i = num_hits / i
            sum_precisions += precision_at_i

    # Normalize by total relevant docs (always 3 for ultra validation)
    ap = sum_precisions / len(ground_truth) if ground_truth else 0.0

    return ap

def count_hits(ground_truth, predicted):
    """Count how many predicted docs are in ground truth"""
    ground_truth_set = set(ground_truth)
    hits = sum(1 for doc in predicted[:3] if doc in ground_truth_set)
    return hits

def evaluate_cascaded_reranking(validation_data, embeddings_dict):
    """
    Run cascaded reranking on each validation sample

    For each sample:
    - Extract eval_id, query, ground_truth
    - Run cascaded_reranking_strategy(eval_id, query, embeddings_dict)
    - Get predicted top-3 docids
    - Calculate AP@3 for this query
    - Store detailed results
    """
    print("\nRunning Cascaded Reranking v1 on validation samples...")
    print("="*80)

    results = []

    for val_item in validation_data:
        eval_id = val_item['eval_id']
        query = val_item['query']  # List of role/content dicts
        ground_truth = val_item['ground_truth']  # List of 3 docids

        print(f"\n[{len(results)+1}/3] Processing eval_id={eval_id}...")

        # Run cascaded reranking
        predicted_topk = cascaded_reranking_strategy(eval_id, query, embeddings_dict)

        # Calculate AP@3
        ap = calculate_ap_at_3(ground_truth, predicted_topk)

        # Extract query text for display
        query_text = query[-1]['content'] if query else ''

        results.append({
            'eval_id': eval_id,
            'query_text': query_text,
            'rewritten_query': val_item.get('rewritten_query', ''),
            'ground_truth': ground_truth,
            'predicted': predicted_topk,
            'ap': ap,
            'hits': count_hits(ground_truth, predicted_topk)
        })

        print(f"  â†’ AP@3: {ap:.4f} | Hits: {results[-1]['hits']}/3")

    return results

def print_detailed_results(results):
    """
    Print detailed results for each query:
    - Query text
    - Ground truth docids
    - Predicted docids
    - Hits/misses
    - AP score
    """
    print("\n" + "="*80)
    print("ULTRA VALIDATION RESULTS - Cascaded Reranking v1")
    print("="*80)

    for i, result in enumerate(results, 1):
        print(f"\n{'='*80}")
        print(f"[{i}] eval_id={result['eval_id']}")
        print(f"{'='*80}")
        print(f"Query: {result['query_text']}")
        print(f"\nAP@3: {result['ap']:.4f} | Hits: {result['hits']}/3")

        print(f"\nGround Truth (Solar Pro 5-Phase Validated):")
        for j, docid in enumerate(result['ground_truth'], 1):
            print(f"  {j}. {docid}")

        print(f"\nPredicted (Cascaded Reranking v1):")
        for j, docid in enumerate(result['predicted'], 1):
            hit = "âœ“" if docid in result['ground_truth'] else "âœ—"
            print(f"  {j}. {docid} {hit}")

    # Overall MAP@3
    map_score = sum(r['ap'] for r in results) / len(results) if results else 0.0

    print(f"\n{'='*80}")
    print(f"OVERALL PERFORMANCE")
    print(f"{'='*80}")
    print(f"MAP@3 on Ultra Validation: {map_score:.4f}")
    print(f"Current Best (query_expansion_v1): 0.7848")
    print(f"{'='*80}")

    if map_score > 0.7848:
        improvement = map_score - 0.7848
        print(f"âœ… IMPROVEMENT: +{improvement:.4f} ({improvement/0.7848*100:.1f}%)")
        print(f"{'='*80}")
        print(f"ðŸŽ¯ RECOMMENDATION: Cascaded reranking performs BETTER than baseline!")
        print(f"   â†’ Submit to competition: cascaded_reranking_v1_submission.csv")
        print(f"   â†’ Expand validation set to confirm improvement")
    elif abs(map_score - 0.7848) < 0.05:  # Within 5% (0.05 absolute difference)
        diff = abs(map_score - 0.7848)
        print(f"â‰ˆ  SIMILAR: Â±{diff:.4f} ({diff/0.7848*100:.1f}%)")
        print(f"{'='*80}")
        print(f"ðŸ¤” RECOMMENDATION: Performance is SIMILAR to baseline")
        print(f"   â†’ Validation approach seems reliable")
        print(f"   â†’ Expand validation set (lower Phase 2 threshold to 3)")
        print(f"   â†’ Generate 30-50 samples for better statistical confidence")
    else:
        decline = 0.7848 - map_score
        print(f"âŒ DECLINE: -{decline:.4f} ({decline/0.7848*100:.1f}%)")
        print(f"{'='*80}")
        print(f"âš ï¸  RECOMMENDATION: Performance is WORSE than baseline")
        print(f"   â†’ Review Solar Pro's labeling criteria")
        print(f"   â†’ Investigate discrepancy between validation and real performance")
        print(f"   â†’ May need to adjust validation pipeline")

    print(f"{'='*80}\n")

    return map_score

def save_validation_results(results, output_path='ultra_validation_results.json'):
    """Save detailed results to JSON file"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({
            'results': results,
            'map_score': sum(r['ap'] for r in results) / len(results) if results else 0.0,
            'baseline_map': 0.7848,
            'num_samples': len(results)
        }, f, ensure_ascii=False, indent=2)
    print(f"ðŸ’¾ Detailed results saved to: {output_path}")

def main():
    print("\n" + "="*80)
    print("Ultra Validation Test: Cascaded Reranking v1")
    print("="*80)
    print("Testing on 3 ultra-high-quality samples from Solar Pro 5-Phase Pipeline")
    print("="*80)

    # Load validation data
    print("\nðŸ“‚ Loading ultra validation samples...")
    validation_data = load_ultra_validation('ultra_validation_solar.jsonl')

    # Run cascaded reranking
    results = evaluate_cascaded_reranking(validation_data, embeddings_dict)

    # Print detailed results
    map_score = print_detailed_results(results)

    # Save results
    save_validation_results(results)

    print("\nâœ… Validation complete!")
    print(f"MAP@3: {map_score:.4f} (baseline: 0.7848)")
    print("="*80 + "\n")

    return map_score, results

if __name__ == "__main__":
    map_score, results = main()
