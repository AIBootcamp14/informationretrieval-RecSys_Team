"""
Query Expansion v1: Phase 1 ì‹¤í—˜
- LLM ì¿¼ë¦¬ ì¬ì‘ì„± (rag_with_context.py í™œìš©)
- BGE-M3 ì¿¼ë¦¬ ê¸¸ì´ ìµœì í™” (64â†’128)
- Enhanced PRF with LLM

ëª©í‘œ: 0.83+ MAP@3
"""

import json
import os
import pickle
import numpy as np
from tqdm import tqdm
from elasticsearch import Elasticsearch
from FlagEmbedding import BGEM3FlagModel
from openai import OpenAI

# ES ì—°ê²°
es = Elasticsearch(['http://localhost:9200'])

# Solar API ì´ˆê¸°í™”
upstage_api_key = os.environ.get('UPSTAGE_API_KEY')
client = None
if upstage_api_key:
    client = OpenAI(
        api_key=upstage_api_key,
        base_url="https://api.upstage.ai/v1/solar"
    )

# BGE-M3 ëª¨ë¸ ë¡œë“œ
print("BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
print("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ì¼ë°˜ ëŒ€í™” ID
SMALLTALK_IDS = {276, 261, 233, 90, 222, 235, 165, 153, 169, 141, 183}

# BGE-M3 ìµœì í™” ì„ë² ë”© ë¡œë“œ
print("\nBGE-M3 ìµœì í™” ì„ë² ë”© ë¡œë“œ ì¤‘...")
with open('embeddings_test_bgem3_optimized.pkl', 'rb') as f:
    embeddings_dict = pickle.load(f)
print(f"âœ… {len(embeddings_dict)}ê°œ ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")

def rewrite_query_with_context(msg):
    """
    ë©€í‹°í„´ ëŒ€í™”ì˜ ë§¥ë½ì„ í†µí•©í•˜ì—¬ ì¿¼ë¦¬ ì¬ì‘ì„± (rag_with_context.py ê¸°ë°˜)
    """
    if isinstance(msg, str):
        return msg

    if len(msg) == 1:
        return msg[0]['content']

    current_query = msg[-1]['content']

    # ëŒ€ëª…ì‚¬ë‚˜ ëª¨í˜¸í•œ í‘œí˜„ í™•ì¸
    ambiguous_terms = ['ê·¸ ', 'ê·¸ê²ƒ', 'ì´ê²ƒ', 'ì´ê±°', 'ì €ê²ƒ', 'ì €ê±°', 'ì™œ', 'ì–´ë–»ê²Œ', 'ì´ìœ ']

    if not any(term in current_query for term in ambiguous_terms):
        return current_query

    if not client:
        return current_query

    # LLMìœ¼ë¡œ ì¿¼ë¦¬ ì¬ì‘ì„±
    conversation_context = "\n".join([
        f"{m['role']}: {m['content']}" for m in msg[:-1]
    ])

    prompt = f"""ë‹¤ìŒì€ ì´ì „ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:

{conversation_context}

í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{current_query}"

ì´ ì§ˆë¬¸ì„ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ë°˜ì˜í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
ëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ ë“±)ë¥¼ êµ¬ì²´ì ì¸ ëª…ì‚¬ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.

ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="solar-pro",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=150
        )

        rewritten = response.choices[0].message.content.strip()
        rewritten = rewritten.strip('"').strip("'")

        return rewritten

    except Exception as e:
        return current_query

def expand_query_with_prf(query, top_docs, top_k=3):
    """
    Enhanced PRF with LLM
    ìƒìœ„ Kê°œ ë¬¸ì„œë¡œë¶€í„° LLMì´ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    if not client or not top_docs or len(top_docs) < top_k:
        return query

    # ìƒìœ„ Kê°œ ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    doc_contents = []
    for doc in top_docs[:top_k]:
        content_preview = doc['content'][:200]
        doc_contents.append(content_preview)

    docs_text = "\n\n".join(doc_contents)

    prompt = f"""ë‹¤ìŒì€ ê²€ìƒ‰ëœ ìƒìœ„ ë¬¸ì„œë“¤ì˜ ì¼ë¶€ì…ë‹ˆë‹¤:

{docs_text}

ì›ë˜ ì§ˆë¬¸: "{query}"

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì›ë˜ ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ ì¶”ê°€ í‚¤ì›Œë“œë‚˜ ë™ì˜ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.

í™•ì¥ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì›ë˜ ì§ˆë¬¸ + ê´€ë ¨ í‚¤ì›Œë“œ)"""

    try:
        response = client.chat.completions.create(
            model="solar-pro",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=200
        )

        expanded = response.choices[0].message.content.strip()
        expanded = expanded.strip('"').strip("'")

        return expanded

    except Exception as e:
        return query

def search_bm25(query, top_k=20):
    """BM25 ê²€ìƒ‰"""
    fetch_size = top_k + 5

    response = es.search(
        index='test',
        body={
            'query': {
                'match': {
                    'content': {
                        'query': query,
                        'analyzer': 'nori'
                    }
                }
            },
            'size': fetch_size
        }
    )

    if not response['hits']['hits']:
        return []

    # original_docid ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    seen_original_docids = set()
    results = []

    for hit in response['hits']['hits']:
        source = hit['_source']
        original_docid = source.get('original_docid', source['docid'])

        if original_docid in seen_original_docids:
            continue

        seen_original_docids.add(original_docid)
        results.append({
            'docid': original_docid,
            'content': source['content'],
            'score': hit['_score'],
            'source': 'bm25'
        })

        if len(results) >= top_k:
            break

    return results

def bgem3_hybrid_score(query_dense, query_sparse, query_colbert,
                       doc_dense, doc_sparse, doc_colbert,
                       w1=0.4, w2=0.3, w3=0.3):
    """BGE-M3 Hybrid Scoring"""
    # 1. Dense ìœ ì‚¬ë„
    s_dense = np.dot(query_dense, doc_dense) / (
        np.linalg.norm(query_dense) * np.linalg.norm(doc_dense)
    )

    # 2. Sparse ìœ ì‚¬ë„
    s_lex = 0.0
    if query_sparse and doc_sparse:
        common_tokens = set(query_sparse.keys()) & set(doc_sparse.keys())
        for token in common_tokens:
            s_lex += query_sparse[token] * doc_sparse[token]

    # 3. ColBERT ìœ ì‚¬ë„
    s_mul = 0.0
    if query_colbert.shape[0] > 0 and doc_colbert.shape[0] > 0:
        query_colbert_norm = query_colbert / np.linalg.norm(query_colbert, axis=1, keepdims=True)
        doc_colbert_norm = doc_colbert / np.linalg.norm(doc_colbert, axis=1, keepdims=True)
        sim_matrix = np.dot(query_colbert_norm, doc_colbert_norm.T)
        s_mul = np.mean(np.max(sim_matrix, axis=1))

    hybrid_score = w1 * s_dense + w2 * s_lex + w3 * s_mul
    return hybrid_score

def search_bgem3_hybrid(query, embeddings_dict, top_k=20, max_length=128):
    """
    BGE-M3 Hybrid ê²€ìƒ‰ (ì¿¼ë¦¬ ê¸¸ì´ ìµœì í™”)
    max_length: 64 â†’ 128 (Query Expansionìœ¼ë¡œ ì¿¼ë¦¬ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŒ)
    """
    # BGE-M3 ì¿¼ë¦¬ ì„ë² ë”© (ìµœì í™”ëœ max_length)
    query_embedding = model.encode(
        [query],
        return_dense=True,
        return_sparse=True,
        return_colbert_vecs=True,
        max_length=max_length  # âœ… 64 â†’ 128
    )

    query_dense = query_embedding['dense_vecs'][0]
    query_sparse = query_embedding['lexical_weights'][0]
    query_colbert = query_embedding['colbert_vecs'][0]

    # ëª¨ë“  ë¬¸ì„œì— ëŒ€í•´ Hybrid Score ê³„ì‚°
    scores = []
    for docid, doc_emb in embeddings_dict.items():
        score = bgem3_hybrid_score(
            query_dense, query_sparse, query_colbert,
            doc_emb['dense'], doc_emb['sparse'], doc_emb['colbert']
        )
        scores.append((docid, score))

    # ì •ë ¬
    scores.sort(key=lambda x: x[1], reverse=True)

    # ESì—ì„œ content ê°€ì ¸ì˜¤ê¸°
    results = []
    for docid, score in scores[:top_k]:
        try:
            resp = es.search(
                index='test',
                body={
                    'query': {
                        'bool': {
                            'should': [
                                {'term': {'docid.keyword': docid}},
                                {'term': {'original_docid.keyword': docid}}
                            ]
                        }
                    },
                    'size': 1
                }
            )

            if resp['hits']['hits']:
                source = resp['hits']['hits'][0]['_source']
                results.append({
                    'docid': docid,
                    'content': source['content'],
                    'score': float(score),
                    'source': 'bgem3_hybrid'
                })
        except Exception as e:
            continue

    return results

def hybrid_search_rrf(query, embeddings_dict, top_k=20, k=60, query_max_length=128):
    """RRFë¡œ BM25 + BGE-M3 Hybrid ê²°í•©"""
    # BM25 ê²€ìƒ‰
    bm25_results = search_bm25(query, top_k=top_k)

    # BGE-M3 Hybrid ê²€ìƒ‰ (ìµœì í™”ëœ ì¿¼ë¦¬ ê¸¸ì´)
    bgem3_results = search_bgem3_hybrid(query, embeddings_dict, top_k=top_k, max_length=query_max_length)

    # RRF ìŠ¤ì½”ì–´ ê³„ì‚°
    rrf_scores = {}
    doc_contents = {}

    for rank, doc in enumerate(bm25_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + 1 / (k + rank)
        doc_contents[docid] = doc['content']

    for rank, doc in enumerate(bgem3_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + 1 / (k + rank)
        doc_contents[docid] = doc['content']

    # ì •ë ¬
    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

    results = []
    for docid, score in sorted_docs:
        results.append({
            'docid': docid,
            'content': doc_contents[docid],
            'score': score
        })

    return results

def llm_rerank(query, docs, top_k=3):
    """Solar-proë¡œ Reranking"""
    if not docs or len(docs) <= top_k or not client:
        return [doc['docid'] for doc in docs[:top_k]]

    try:
        doc_list = []
        for i, doc in enumerate(docs[:15]):
            content_preview = doc['content'][:300]
            if len(doc['content']) > 300:
                content_preview += "..."
            doc_list.append(f"[{i}] {content_preview}")

        docs_text = "\n\n".join(doc_list)

        response = client.chat.completions.create(
            model="solar-pro",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ê³¼í•™ ì§€ì‹ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ relevance íŒë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì •í™•íˆ ì„ íƒí•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"""ì¿¼ë¦¬: {query}

ë¬¸ì„œë“¤:
{docs_text}

ì´ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ {top_k}ê°œì˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.
- ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ”ê°€?
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ê°€?
- ê³¼í•™ì ìœ¼ë¡œ ì •í™•í•œê°€?

ì¶œë ¥: ë²ˆí˜¸ë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„ (ì˜ˆ: 0,2,4)
ì„¤ëª… ì—†ì´ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
                }
            ],
            temperature=0.0,
            max_tokens=30
        )

        result = response.choices[0].message.content.strip()
        indices = [int(x.strip()) for x in result.split(',') if x.strip().isdigit()]

        reranked_docids = []
        for idx in indices[:top_k]:
            if 0 <= idx < len(docs):
                reranked_docids.append(docs[idx]['docid'])

        # ë¶€ì¡±í•˜ë©´ ì›ë˜ ìˆœì„œë¡œ ì±„ìš°ê¸°
        for doc in docs:
            if len(reranked_docids) >= top_k:
                break
            if doc['docid'] not in reranked_docids:
                reranked_docids.append(doc['docid'])

        return reranked_docids[:top_k]

    except Exception as e:
        return [doc['docid'] for doc in docs[:top_k]]

def query_expansion_strategy(eval_id, msg, embeddings_dict, use_prf=False):
    """
    Query Expansion v1 ì „ëµ

    1. LLM ì¿¼ë¦¬ ì¬ì‘ì„± (ë©€í‹°í„´ ëŒ€í™” ë§¥ë½ í†µí•©)
    2. BGE-M3 ì¿¼ë¦¬ ê¸¸ì´ ìµœì í™” (128)
    3. (ì„ íƒ) Enhanced PRF with LLM
    """
    # ì¼ë°˜ ëŒ€í™”ëŠ” ë¹ˆ ê²°ê³¼
    if eval_id in SMALLTALK_IDS:
        return []

    # Step 1: ì¿¼ë¦¬ ì¬ì‘ì„±
    rewritten_query = rewrite_query_with_context(msg)

    # Step 2: Hybrid Search (ì¿¼ë¦¬ ê¸¸ì´ 128)
    hybrid_results = hybrid_search_rrf(
        rewritten_query,
        embeddings_dict,
        top_k=20,
        k=60,
        query_max_length=128  # âœ… ìµœì í™”
    )

    if not hybrid_results:
        return []

    # Step 3: (ì„ íƒ) Enhanced PRF
    if use_prf:
        expanded_query = expand_query_with_prf(rewritten_query, hybrid_results, top_k=3)

        # PRF ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰
        hybrid_results = hybrid_search_rrf(
            expanded_query,
            embeddings_dict,
            top_k=20,
            k=60,
            query_max_length=128
        )

    # Step 4: LLM Reranking
    final_topk = llm_rerank(rewritten_query, hybrid_results, top_k=3)

    return final_topk

def run_experiment(use_prf=False):
    """Query Expansion v1 ì‹¤í—˜ ì‹¤í–‰"""
    print("="*80)
    print("Query Expansion v1 ì‹¤í—˜")
    print("="*80)
    print(f"Phase 1: LLM ì¿¼ë¦¬ ì¬ì‘ì„± + BGE-M3 ê¸¸ì´ ìµœì í™” (128)")
    print(f"Enhanced PRF: {'ON' if use_prf else 'OFF'}")
    print("="*80)

    # Eval ë°ì´í„° ë¡œë“œ
    with open('../data/eval.jsonl', 'r', encoding='utf-8') as f:
        eval_data = [json.loads(line) for line in f]

    print(f"\nğŸ“‹ ì´ {len(eval_data)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘\n")

    results = []

    for item in tqdm(eval_data, desc="Query Expansion v1"):
        eval_id = item['eval_id']
        msg = item['msg']

        # Query Expansion ì „ëµ ì‹¤í–‰
        topk = query_expansion_strategy(eval_id, msg, embeddings_dict, use_prf=use_prf)

        results.append({
            'eval_id': eval_id,
            'retrieve': topk
        })

    # ì œì¶œ íŒŒì¼ ìƒì„±
    prf_suffix = "_prf" if use_prf else ""
    output_path = f'query_expansion_v1{prf_suffix}_submission.csv'
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in results:
            json_obj = {
                'eval_id': r['eval_id'],
                'topk': r['retrieve']
            }
            f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')

    print(f"\n{'='*80}")
    print(f"âœ… ì‹¤í—˜ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ğŸ’¾ ì œì¶œ íŒŒì¼: {output_path}")
    print(f"{'='*80}")

    return results

if __name__ == "__main__":
    # Experiment 1: LLM ì¿¼ë¦¬ ì¬ì‘ì„± + BGE-M3 ê¸¸ì´ ìµœì í™”
    print("\n" + "="*80)
    print("Experiment 1: ê¸°ë³¸ (ì¿¼ë¦¬ ì¬ì‘ì„± + ê¸¸ì´ ìµœì í™”)")
    print("="*80)
    run_experiment(use_prf=False)

    # Experiment 2: + Enhanced PRF
    print("\n" + "="*80)
    print("Experiment 2: + Enhanced PRF")
    print("="*80)
    run_experiment(use_prf=True)
