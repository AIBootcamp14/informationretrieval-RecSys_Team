"""
LLM Maximum vs LLM Optimized ì„±ëŠ¥ ë¹„êµ ë¶„ì„
"""

import json
from collections import Counter

def load_results(filepath):
    """ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return [json.loads(line) for line in f]

def analyze_submission(filepath, name):
    """ê²°ê³¼ ë¶„ì„"""
    with open(filepath, 'r', encoding='utf-8') as f:
        results = [json.loads(line) for line in f]

    topk_counts = {}
    for r in results:
        k = len(r['topk'])
        topk_counts[k] = topk_counts.get(k, 0) + 1

    print(f'\n{"="*80}')
    print(f'{name}')
    print(f'{"="*80}')
    print(f'Total queries: {len(results)}')
    print(f'\nğŸ“Š TopK Distribution:')
    for k in sorted(topk_counts.keys()):
        count = topk_counts[k]
        pct = count / len(results) * 100
        print(f'  TopK={k}: {count:3d} ({pct:5.1f}%)')

    with_results = sum(1 for r in results if len(r['topk']) > 0)
    print(f'\nâœ… Results provided: {with_results}/{len(results)} ({with_results/len(results)*100:.1f}%)')

    avg_docs = sum(len(r['topk']) for r in results) / len(results)
    print(f'ğŸ“ˆ Average documents: {avg_docs:.2f}')

    return {
        'results': results,
        'topk_counts': topk_counts,
        'total': len(results),
        'with_results': with_results,
        'avg_docs': avg_docs
    }

def compare_results(baseline_data, maximum_data):
    """ë‘ ê²°ê³¼ ë¹„êµ"""
    print(f'\n{"="*80}')
    print(f'ğŸ” ìƒì„¸ ë¹„êµ ë¶„ì„')
    print(f'{"="*80}')

    baseline_results = baseline_data['results']
    maximum_results = maximum_data['results']

    # eval_id ìˆœì„œ ë§ì¶”ê¸°
    baseline_dict = {r['eval_id']: r['topk'] for r in baseline_results}
    maximum_dict = {r['eval_id']: r['topk'] for r in maximum_results}

    # ë™ì¼ ì¿¼ë¦¬ ë¹„êµ
    same_count = 0
    different_count = 0
    baseline_better = 0  # Baselineë§Œ ê²°ê³¼ ìˆìŒ
    maximum_better = 0  # Maximumë§Œ ê²°ê³¼ ìˆìŒ

    differences = []

    for eval_id in baseline_dict.keys():
        if eval_id not in maximum_dict:
            continue

        baseline_topk = baseline_dict[eval_id]
        maximum_topk = maximum_dict[eval_id]

        # ì™„ì „íˆ ë™ì¼í•œì§€ í™•ì¸
        if baseline_topk == maximum_topk:
            same_count += 1
        else:
            different_count += 1
            differences.append({
                'eval_id': eval_id,
                'baseline': baseline_topk,
                'maximum': maximum_topk
            })

        # ì–´ëŠ ìª½ì´ ë” ë§ì€ ê²°ê³¼ë¥¼ ì œê³µí–ˆëŠ”ì§€
        if len(baseline_topk) > 0 and len(maximum_topk) == 0:
            baseline_better += 1
        elif len(baseline_topk) == 0 and len(maximum_topk) > 0:
            maximum_better += 1

    print(f'\nğŸ”„ ê²°ê³¼ ì¼ì¹˜ë„:')
    total_compared = same_count + different_count
    print(f'  ë¹„êµëœ ì¿¼ë¦¬: {total_compared}ê°œ')
    print(f'  ë™ì¼í•œ ê²°ê³¼: {same_count}ê°œ ({same_count/total_compared*100:.1f}%)')
    print(f'  ë‹¤ë¥¸ ê²°ê³¼: {different_count}ê°œ ({different_count/total_compared*100:.1f}%)')

    print(f'\nâš–ï¸  ê²°ê³¼ ì œê³µ ëŠ¥ë ¥:')
    print(f'  Baselineë§Œ ê²°ê³¼ ìˆìŒ: {baseline_better}ê°œ')
    print(f'  Maximumë§Œ ê²°ê³¼ ìˆìŒ: {maximum_better}ê°œ')

    # TopK ë¶„í¬ ë¹„êµ
    print(f'\nğŸ“Š TopK ë¶„í¬ ë³€í™”:')
    print(f'  {"TopK":<10} {"Baseline":<15} {"Maximum":<15} {"ì°¨ì´"}')
    print(f'  {"-"*60}')
    for k in sorted(set(list(baseline_data['topk_counts'].keys()) + list(maximum_data['topk_counts'].keys()))):
        baseline_count = baseline_data['topk_counts'].get(k, 0)
        maximum_count = maximum_data['topk_counts'].get(k, 0)
        diff = maximum_count - baseline_count
        diff_str = f'{diff:+d}' if diff != 0 else '0'
        print(f'  TopK={k:<5} {baseline_count:<15} {maximum_count:<15} {diff_str}')

def main():
    print("="*80)
    print("LLM Maximum vs LLM Optimized ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
    print("="*80)

    # íŒŒì¼ ë¡œë“œ ë° ë¶„ì„
    print("\nğŸ“‚ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ë° ë¶„ì„ ì¤‘...")

    try:
        baseline_data = analyze_submission(
            'Solar_optimized_submission.csv',
            'Baseline: LLM Optimized (Score: 0.6856) - 2 LLM Stages'
        )
    except Exception as e:
        print(f"âŒ Baseline ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    try:
        maximum_data = analyze_submission(
            'llm_maximum_submission.csv',
            'LLM Maximum - 6 LLM Stages'
        )
    except Exception as e:
        print(f"âŒ LLM Maximum ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # ë¹„êµ ë¶„ì„
    compare_results(baseline_data, maximum_data)

    # ì¢…í•© ìš”ì•½
    print(f'\n{"="*80}')
    print(f'ğŸ“Š ì¢…í•© ìš”ì•½')
    print(f'{"="*80}')

    print(f'\nğŸ¯ ê²°ê³¼ ì œê³µë¥  ë¹„êµ:')
    print(f'  Baseline: {baseline_data["with_results"]}/{baseline_data["total"]} ({baseline_data["with_results"]/baseline_data["total"]*100:.1f}%)')
    print(f'  Maximum:  {maximum_data["with_results"]}/{maximum_data["total"]} ({maximum_data["with_results"]/maximum_data["total"]*100:.1f}%)')

    print(f'\nğŸ“ í‰ê·  ë¬¸ì„œ ìˆ˜:')
    print(f'  Baseline: {baseline_data["avg_docs"]:.2f}ê°œ')
    print(f'  Maximum:  {maximum_data["avg_docs"]:.2f}ê°œ')

    print(f'\nğŸš€ LLM Maximum ì£¼ìš” ê°œì„ ì‚¬í•­:')
    print(f'  âœ… LLM Intent Classification (í•˜ë“œì½”ë”© â†’ LLM)')
    print(f'  âœ… Context-Aware Rewriting (ë‹¤ì¤‘ í„´ ëŒ€í™” ì²˜ë¦¬)')
    print(f'  âœ… LLM Query Enhancement (í‚¤ì›Œë“œ í™•ì¥)')
    print(f'  âœ… LLM Document Relevance Scoring (0-100ì  í‰ê°€)')
    print(f'  âœ… LLM Final Reranking (70ì  ì´ìƒë§Œ ì„ íƒ)')

    print(f'\n{"="*80}')

if __name__ == "__main__":
    main()
