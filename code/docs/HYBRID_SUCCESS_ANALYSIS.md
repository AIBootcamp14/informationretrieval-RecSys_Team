# Hybrid 솔루션 0.7523 성능 분석 보고서

## 📊 핵심 성과 요약

### 점수 비교
- **Hybrid (최고)**: 0.7523 (실제 제출)
- **BM25 Only**: ~0.6-0.7 (이전 최고)
- **개선폭**: +0.05~0.15 점 (7-20% 향상)

### 로컬 검증 vs 실제 점수
- **로컬 MAP@3**: 0.9442 (LLM 생성 ground truth 기준)
- **실제 MAP@3**: 0.7523
- **차이 원인**: LLM이 생성한 ground truth가 실제 정답과 다름 (LLM 편향)

## 🔍 구체적 개선 사례 분석

### 1. BM25에서 실패했던 쿼리들이 Hybrid에서 성공

#### Case 1: eval_id=35
- **BM25 결과**: AP=0.000 (완전 실패)
  - Retrieved: ['2d86c0ab...', '068de76f...', 'befe70d8...']
  - GT: ['2e161dc4...', '4392ecba...']
  - 문제: 정답 2개 중 0개 찾음

- **Hybrid 결과**: AP=0.500 (50% 성공)
  - Retrieved: ['4392ecba...', 'c6f86dc0...', '2d86c0ab...']
  - GT: ['4392ecba...', '2e161dc4...']
  - 개선: 정답 1개 찾음 (1위에 배치)

**개선 이유**: Dense retrieval이 의미적 유사성으로 '4392ecba' 문서 찾음 → RRF로 상위 랭크 → LLM이 1위로 선택

#### Case 2: eval_id=69
- **BM25 결과**: AP=0.000
  - Retrieved: ['1853000e...', '5966d4b0...', '6d5d619b...']
  - GT: ['679346ae...', '176c3a3a...']

- **Hybrid 결과**: AP=0.500
  - Retrieved: ['176c3a3a...', 'f9be82b8...', 'f9be82b8...']
  - GT: ['679346ae...', '176c3a3a...']

**개선 이유**: Dense + LLM이 키워드 매칭 실패를 의미 기반으로 보완

#### Case 3: eval_id=72
- **BM25 결과**: AP=0.000
  - Retrieved: ['c8fd4323...'] (3번 중복)
  - GT: ['1a277fb7...']

- **Hybrid 결과**: AP=0.500
  - Retrieved: ['c8fd4323...', '1a277fb7...', '7619cd24...']
  - GT: ['1a277fb7...']

**개선 이유**: 중복 제거 개선 + Dense가 다양한 후보 제공

### 2. 숫자로 본 명확한 개선

#### BM25 Only (bm25_k15_b50_submission.csv)
```
AP=0.0:       20개 (15.9%)  ← 완전 실패
0.0<AP≤0.3:   15개 (11.9%)
0.3<AP≤0.6:   22개 (17.5%)
0.6<AP≤0.9:   11개 (8.7%)
AP>0.9:       58개 (46.0%)  ← 거의 완벽
총 MAP@3:     0.8161
```

#### Hybrid Final (hybrid_final_submission_fixed.csv)
```
AP=0.0:        1개 (0.8%)   ← 20개 → 1개로 감소 (95% 개선)
0.0<AP≤0.3:    3개 (2.4%)   ← 15개 → 3개로 감소 (80% 개선)
0.3<AP≤0.6:   11개 (8.7%)   ← 22개 → 11개로 감소 (50% 개선)
0.6<AP≤0.9:    5개 (4.0%)
AP>0.9:      106개 (84.1%)  ← 58개 → 106개로 증가 (83% 증가)
총 MAP@3:     0.9442
```

**핵심 개선 지표**:
- **완전 실패(AP=0) 쿼리**: 20개 → 1개 (95% 감소)
- **거의 완벽(AP>0.9) 쿼리**: 58개 → 106개 (83% 증가)
- **로컬 MAP@3**: 0.8161 → 0.9442 (+0.13점, 16% 향상)

## 🎯 각 컴포넌트의 기여도 분석

### 1. BM25 (Keyword Matching)
- **역할**: 키워드 기반 1차 필터링
- **강점**: 정확한 용어 매칭 (예: "광합성", "DNA 복제")
- **약점**: 동의어, 의미적 유사성 포착 불가
- **기여도**: 기본 recall 확보 (~60-70% 쿼리에서 정답 포함)

### 2. Dense Retrieval (Semantic Search)
- **역할**: 의미적 유사성 기반 보완
- **강점**: BM25가 놓친 의미적으로 유사한 문서 발견
- **약점**: 단독으로는 정밀도 낮음
- **기여도**: BM25 실패 케이스 보완 (완전 실패 20개 → 1개)

**구체적 예시**:
- eval_id=35: BM25 놓친 '4392ecba' 문서를 Dense가 발견
- eval_id=69: 키워드 매칭 실패했지만 Dense가 '176c3a3a' 찾음

### 3. RRF (Reciprocal Rank Fusion)
- **역할**: BM25 + Dense 결과 융합
- **파라미터**: k=60
- **강점**: 단순하지만 효과적인 융합 (가중치 튜닝 불필요)
- **기여도**: 두 방법의 장점 결합, 순위 안정화

**RRF 공식**:
```
RRF_score = 1/(k + rank_bm25) + 1/(k + rank_dense)
```

### 4. LLM Reranking (Solar-pro)
- **역할**: 상위 15개 후보에서 최종 Top-3 선택
- **강점**:
  - 문맥 이해 능력
  - Relevance 판단 정확도
  - 과학 지식 활용
- **기여도**: 정밀도 대폭 향상 (AP>0.9 비율 46% → 84%)

**LLM이 개선한 케이스**:
- eval_id=35: RRF 후보 중 가장 관련성 높은 '4392ecba'를 1위로 배치
- eval_id=72: 중복 제거 후 올바른 순서로 재배열

## 💡 왜 0.7523이 달성되었는가?

### 1. 다층 구조의 시너지 효과

```
Query
  ↓
[BM25: 키워드 매칭] ─┐
                      ├─→ [RRF: 융합] → [LLM: 최종 판단] → Top-3
[Dense: 의미 검색] ──┘
```

각 단계가 이전 단계의 약점을 보완:
1. BM25가 키워드로 1차 필터링
2. Dense가 의미적으로 유사한 문서 추가
3. RRF가 두 결과를 균형있게 융합
4. LLM이 최종적으로 가장 관련성 높은 문서 선택

### 2. 한국어 최적화
- **Nori Analyzer**: 한국어 형태소 분석 (BM25)
- **ko-sroberta-multitask**: 한국어 의미 임베딩 (Dense)
- **Solar-pro**: 한국어 LLM (Reranking)

### 3. 로컬 검증 시스템
- **LLM 기반 Ground Truth**: 220개 쿼리 자동 라벨링
- **빠른 반복**: API 제출 없이 즉시 성능 확인
- **실험 자동화**: batch_validate.py로 여러 전략 동시 평가

## 🚧 남은 Gap 분석: 0.7523 → 0.9+

### 로컬 vs 실제 점수 차이

**로컬**: 0.9442 (LLM 생성 GT)
**실제**: 0.7523 (사람이 만든 GT)
**Gap**: 0.19점

### 차이 원인 가설

1. **LLM Ground Truth 편향**
   - LLM이 생성한 정답 ≠ 실제 정답
   - LLM이 Solar-pro 관점에서 라벨링 → 같은 LLM이 reranking → 과적합

2. **실패하는 쿼리 유형**
   - eval_id=306: AP=0.0 (로컬에서도 실패)
   - GT: ['3ab69b86...', 'bb33fdbb...']
   - Retrieved: ['dafabda1...', 'a8925d82...', '12e80aff...']
   - 이유: BM25/Dense 모두 관련 문서를 상위에 못 올림

3. **개선 여지**
   - **BM25/Dense Top-K 증가**: 20 → 30~50
   - **LLM 후보 수 증가**: 15 → 20~30
   - **RRF k값 조정**: 60 → 30~90
   - **프롬프트 개선**: 더 명확한 relevance 기준

## 📈 결론: 왜 0.7523이 최고 점수인가?

### 정량적 증거
1. **로컬 검증**: 0.8161 (BM25 only) → 0.9442 (Hybrid) = +0.13점
2. **실제 제출**: ~0.6-0.7 → 0.7523 (Hybrid) = +0.05~0.15점
3. **완전 실패 감소**: 20개 → 1개 (95% 개선)
4. **거의 완벽 증가**: 58개 → 106개 (83% 증가)

### 정성적 이유
1. **BM25 단독의 한계 극복**: 키워드 매칭만으로는 의미적 유사성 포착 불가
2. **Dense의 보완 효과**: BM25가 놓친 의미적으로 유사한 문서 발견
3. **RRF의 융합 효과**: 두 방법의 장점을 균형있게 결합
4. **LLM의 정밀도 향상**: 최종 단계에서 가장 관련성 높은 문서 선택

### 핵심 성공 요인
**"단일 방법이 아닌 다층 구조의 상호 보완"**
- BM25 → Dense → RRF → LLM
- 각 단계가 이전 단계의 약점 보완
- 한국어 특화 모델/분석기 사용
- LLM의 문맥 이해 능력 활용

이제 이 구조를 기반으로 파라미터 최적화를 통해 0.9+ 달성을 목표로 합니다.
