"""
Reliable Validation Setìœ¼ë¡œ Submissions í‰ê°€

ì§„ì§œ ì‚¬ëŒì´ ë ˆì´ë¸”ë§í•œ ë°ì´í„°ë¡œ í‰ê°€
"""

import json
import numpy as np
from collections import defaultdict

def load_validation(path):
    """Validation set ë¡œë“œ"""
    with open(path, 'r') as f:
        return [json.loads(line) for line in f]

def load_submission(path):
    """Submission ë¡œë“œ"""
    with open(path, 'r') as f:
        return {json.loads(line)['eval_id']: json.loads(line) for line in f}

def calculate_average_precision(ground_truth, predicted):
    """
    Average Precision ê³„ì‚°

    Args:
        ground_truth: ì •ë‹µ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        predicted: ì˜ˆì¸¡ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        AP score (0~1)
    """
    if not ground_truth:
        # Smalltalk (ì •ë‹µì´ ì—†ëŠ” ê²½ìš°)
        return 1.0 if len(predicted) == 0 else 0.0

    if not predicted:
        return 0.0

    ap = 0.0
    hits = 0

    for i, pred_doc in enumerate(predicted, 1):
        if pred_doc in ground_truth:
            hits += 1
            precision_at_i = hits / i
            ap += precision_at_i

    if hits == 0:
        return 0.0

    # Normalize by number of relevant documents
    ap /= len(ground_truth)

    return ap

def evaluate_submission(submission_path, validation_path):
    """
    Submissionì„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” Validation setìœ¼ë¡œ í‰ê°€
    """
    print(f"\n{'='*80}")
    print(f"í‰ê°€: {submission_path}")
    print(f"{'='*80}\n")

    # Load data
    val_data = load_validation(validation_path)
    sub_data = load_submission(submission_path)

    # í†µê³„
    stats = {
        'overall': {'total': 0, 'ap_sum': 0, 'perfect': 0},
        'by_confidence': defaultdict(lambda: {'total': 0, 'ap_sum': 0}),
        'by_difficulty': defaultdict(lambda: {'total': 0, 'ap_sum': 0}),
        'smalltalk': {'total': 0, 'correct': 0}
    }

    ap_scores = []
    failures = []

    for val_item in val_data:
        eval_id = val_item['eval_id']

        if eval_id not in sub_data:
            continue

        ground_truth = val_item['ground_truth']
        predicted = sub_data[eval_id]['topk']
        confidence = val_item.get('confidence', 'unknown')
        difficulty = val_item.get('difficulty', 'unknown')

        # Smalltalk ì²˜ë¦¬
        if difficulty == 'smalltalk':
            stats['smalltalk']['total'] += 1
            if len(predicted) == 0:
                stats['smalltalk']['correct'] += 1
            continue

        # AP ê³„ì‚°
        ap = calculate_average_precision(ground_truth, predicted)
        ap_scores.append(ap)

        # Overall
        stats['overall']['total'] += 1
        stats['overall']['ap_sum'] += ap

        if ap == 1.0:
            stats['overall']['perfect'] += 1

        # By confidence
        stats['by_confidence'][confidence]['total'] += 1
        stats['by_confidence'][confidence]['ap_sum'] += ap

        # By difficulty
        stats['by_difficulty'][difficulty]['total'] += 1
        stats['by_difficulty'][difficulty]['ap_sum'] += ap

        # Track failures (AP < 0.5)
        if ap < 0.5:
            failures.append({
                'eval_id': eval_id,
                'query': val_item['query'],
                'ap': ap,
                'ground_truth': ground_truth[:3],
                'predicted': predicted[:3],
                'confidence': confidence,
                'difficulty': difficulty
            })

    # ê²°ê³¼ ì¶œë ¥
    if stats['overall']['total'] > 0:
        overall_map = stats['overall']['ap_sum'] / stats['overall']['total']
        perfect_rate = stats['overall']['perfect'] / stats['overall']['total'] * 100

        print(f"ğŸ“Š Overall Results")
        print(f"  Total queries: {stats['overall']['total']}")
        print(f"  MAP: {overall_map:.4f}")
        print(f"  Perfect (AP=1.0): {stats['overall']['perfect']}ê°œ ({perfect_rate:.1f}%)")
        print(f"  Median AP: {np.median(ap_scores):.4f}")
        print(f"  Min AP: {min(ap_scores):.4f}")
        print(f"  Max AP: {max(ap_scores):.4f}")

    # Confidenceë³„
    print(f"\nğŸ“Š By Confidence Level")
    for conf in ['certain', 'confident', 'uncertain']:
        if stats['by_confidence'][conf]['total'] > 0:
            conf_map = stats['by_confidence'][conf]['ap_sum'] / stats['by_confidence'][conf]['total']
            print(f"  {conf.upper()}: MAP {conf_map:.4f} ({stats['by_confidence'][conf]['total']}ê°œ)")

    # Difficultyë³„
    print(f"\nğŸ“Š By Difficulty")
    for diff in ['easy', 'medium', 'hard']:
        if stats['by_difficulty'][diff]['total'] > 0:
            diff_map = stats['by_difficulty'][diff]['ap_sum'] / stats['by_difficulty'][diff]['total']
            print(f"  {diff.upper()}: MAP {diff_map:.4f} ({stats['by_difficulty'][diff]['total']}ê°œ)")

    # Smalltalk
    if stats['smalltalk']['total'] > 0:
        smalltalk_acc = stats['smalltalk']['correct'] / stats['smalltalk']['total'] * 100
        print(f"\nğŸ“Š Smalltalk")
        print(f"  Accuracy: {smalltalk_acc:.1f}% ({stats['smalltalk']['correct']}/{stats['smalltalk']['total']})")

    # Failures
    if failures:
        print(f"\nâŒ Failures (AP < 0.5): {len(failures)}ê°œ")
        for i, fail in enumerate(failures[:5], 1):
            print(f"\n  [{i}] ID {fail['eval_id']}: {fail['query'][:60]}...")
            print(f"      AP: {fail['ap']:.3f} | Confidence: {fail['confidence']}")
            print(f"      Ground truth: {fail['ground_truth']}")
            print(f"      Predicted: {fail['predicted']}")

    print(f"\n{'='*80}\n")

    return {
        'map': overall_map if stats['overall']['total'] > 0 else 0.0,
        'stats': stats,
        'failures': failures
    }

def compare_submissions(validation_path):
    """
    ì—¬ëŸ¬ submissions ë¹„êµ
    """
    submissions = [
        ('super_simple', 'super_simple_submission.csv', 0.6300),
        ('context_aware', 'context_aware_submission.csv', 0.6220),
        ('selective_context', 'selective_context_submission.csv', 0.6038),
    ]

    print(f"\n{'='*80}")
    print(f"Reliable Validation Setìœ¼ë¡œ Submissions ë¹„êµ")
    print(f"{'='*80}")

    results = []

    for name, path, leaderboard_map in submissions:
        try:
            result = evaluate_submission(path, validation_path)
            results.append({
                'name': name,
                'validation_map': result['map'],
                'leaderboard_map': leaderboard_map,
                'gap': leaderboard_map - result['map']
            })
        except FileNotFoundError:
            print(f"âš ï¸ {path} íŒŒì¼ ì—†ìŒ\n")
        except Exception as e:
            print(f"âš ï¸ {name} í‰ê°€ ì‹¤íŒ¨: {e}\n")

    # ë¹„êµ í…Œì´ë¸”
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ì¢…í•© ë¹„êµ")
    print(f"{'='*80}\n")

    print(f"{'Submission':<25} {'Validation MAP':<15} {'Leaderboard MAP':<18} {'Gap':<10}")
    print(f"{'-'*80}")

    for r in results:
        gap_str = f"{r['gap']:+.4f}"
        print(f"{r['name']:<25} {r['validation_map']:<15.4f} {r['leaderboard_map']:<18.4f} {gap_str:<10}")

    # ìƒê´€ê´€ê³„ ë¶„ì„
    if len(results) >= 2:
        val_maps = [r['validation_map'] for r in results]
        lead_maps = [r['leaderboard_map'] for r in results]

        correlation = np.corrcoef(val_maps, lead_maps)[0, 1]

        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ ìƒê´€ê´€ê³„ ë¶„ì„")
        print(f"{'='*80}")
        print(f"Validation MAP â†” Leaderboard MAP: {correlation:.4f}")

        if correlation > 0.9:
            print(f"âœ… ë§¤ìš° ë†’ì€ ìƒê´€ê´€ê³„ - Validation set ì‹ ë¢° ê°€ëŠ¥!")
        elif correlation > 0.7:
            print(f"âœ… ë†’ì€ ìƒê´€ê´€ê³„ - Validation set ìœ ìš©")
        elif correlation > 0.5:
            print(f"âš ï¸ ì¤‘ê°„ ìƒê´€ê´€ê³„ - ì£¼ì˜ í•„ìš”")
        else:
            print(f"âŒ ë‚®ì€ ìƒê´€ê´€ê³„ - Validation set ì¬ê²€í†  í•„ìš”")

def main():
    print("=" * 80)
    print("Reliable Validation Set ê¸°ë°˜ í‰ê°€")
    print("=" * 80)

    validation_path = 'reliable_validation.jsonl'

    # ë¹„êµ í‰ê°€
    compare_submissions(validation_path)

if __name__ == "__main__":
    main()
