"""
Semantic Chunking for Scientific Korean Text

í•µì‹¬ ê¸°ëŠ¥:
1. ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬ (í•œêµ­ì–´ íŠ¹í™”)
2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í™”
3. ê³¼í•™ ê°œë… ì™„ê²°ì„± ë³´ì¥
"""

import re
import numpy as np
from sentence_transformers import SentenceTransformer

# ì „ì—­ ì„ë² ë”© ëª¨ë¸ (í•œ ë²ˆë§Œ ë¡œë“œ)
_embedding_model = None

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ì‹±ê¸€í†¤"""
    global _embedding_model
    if _embedding_model is None:
        print("ğŸ“¥ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (jhgan/ko-sroberta-multitask)")
        _embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    return _embedding_model

def split_sentences_korean(text):
    """
    í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ (ê·œì¹™ ê¸°ë°˜)

    KoNLPyëŠ” ëŠë¦¬ê³  ê³¼í•™ ìš©ì–´ ì²˜ë¦¬ ë¯¸í¡í•˜ë¯€ë¡œ
    ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì²˜ë¦¬
    """
    if not text or len(text.strip()) == 0:
        return []

    # ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
    # 1. ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ ê³µë°± or ì¤„ë°”ê¿ˆ
    # 2. ë‹¨, ì†Œìˆ˜ì (3.14), ì•½ì–´(Dr.), ìˆœì„œ(1.) ì œì™¸

    # ì„ì‹œ ë³´í˜¸: ìˆ«ì.ìˆ«ì â†’ ìˆ«ì<DOT>ìˆ«ì
    text = re.sub(r'(\d+)\.(\d+)', r'\1<DOT>\2', text)

    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = re.split(r'([.!?])\s+', text)

    # ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ ë‹¤ì‹œ ë¶™ì´ê¸°
    result = []
    i = 0
    while i < len(sentences):
        sent = sentences[i].strip()
        if not sent:
            i += 1
            continue

        # ë‹¤ìŒì´ êµ¬ë‘ì ì´ë©´ ë¶™ì´ê¸°
        if i + 1 < len(sentences) and sentences[i + 1] in '.!?':
            sent = sent + sentences[i + 1]
            i += 2
        else:
            i += 1

        # <DOT> ë³µì›
        sent = sent.replace('<DOT>', '.')

        if len(sent.strip()) > 0:
            result.append(sent.strip())

    return result

def cosine_similarity(emb1, emb2):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))

def semantic_sentence_chunking(document, max_chunk_size=400, min_chunk_size=100, similarity_threshold=0.65, overlap_sentences=0):
    """
    ì˜ë¯¸ì  ë¬¸ì¥ ì²­í‚¹ with Overlap ì§€ì›

    Args:
        document: ì›ë³¸ í…ìŠ¤íŠ¸
        max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
        min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸° (ë„ˆë¬´ ì‘ì€ ì²­í¬ ë°©ì§€)
        similarity_threshold: ì˜ë¯¸ì  ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ê°œë… ì „í™˜ ë¯¼ê°)
        overlap_sentences: ì²­í¬ ê°„ ì¤‘ë³µí•  ë¬¸ì¥ ìˆ˜ (0=ì¤‘ë³µ ì—†ìŒ, 1=ë§ˆì§€ë§‰ 1ë¬¸ì¥ ì¤‘ë³µ ë“±)

    Returns:
        List[str]: ì˜ë¯¸ì ìœ¼ë¡œ ì™„ê²°ëœ ì²­í¬ë“¤
    """
    if not document or len(document.strip()) == 0:
        return []

    # ì§§ì€ ë¬¸ì„œëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
    if len(document) <= max_chunk_size:
        return [document]

    # Step 1: ë¬¸ì¥ ë¶„ë¦¬
    sentences = split_sentences_korean(document)

    if len(sentences) == 0:
        return [document]

    if len(sentences) == 1:
        # ë¬¸ì¥ì´ 1ê°œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return [sentences[0]]

    # Step 2: ê° ë¬¸ì¥ ì„ë² ë”©
    model = get_embedding_model()
    embeddings = model.encode(sentences, convert_to_numpy=True)

    # Step 3: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í™” (with Overlap)
    chunks = []
    current_chunk = [sentences[0]]
    current_length = len(sentences[0])
    overlap_buffer = []  # ë‹¤ìŒ ì²­í¬ì— í¬í•¨í•  ì¤‘ë³µ ë¬¸ì¥ë“¤

    for i in range(1, len(sentences)):
        sent = sentences[i]
        sent_len = len(sent)

        # í˜„ì¬ ì²­í¬ì— ì¶”ê°€ ì‹œ í¬ê¸° ì²´í¬
        if current_length + sent_len <= max_chunk_size:
            # í¬ê¸° OK â†’ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì²´í¬
            prev_emb = embeddings[i - 1]
            curr_emb = embeddings[i]
            similarity = cosine_similarity(prev_emb, curr_emb)

            if similarity >= similarity_threshold:
                # ìœ ì‚¬ë„ ë†’ìŒ â†’ ê°™ì€ ê°œë… â†’ ì¶”ê°€
                current_chunk.append(sent)
                current_length += sent_len
            else:
                # ìœ ì‚¬ë„ ë‚®ìŒ â†’ ê°œë… ì „í™˜ â†’ ìƒˆ ì²­í¬
                # ë‹¨, í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ê°•ì œ ì¶”ê°€
                if current_length < min_chunk_size:
                    current_chunk.append(sent)
                    current_length += sent_len
                else:
                    # ì²­í¬ ì €ì¥
                    chunks.append(' '.join(current_chunk))

                    # Overlap ì²˜ë¦¬: ë§ˆì§€ë§‰ Nê°œ ë¬¸ì¥ì„ ë‹¤ìŒ ì²­í¬ ì‹œì‘ìœ¼ë¡œ
                    if overlap_sentences > 0 and len(current_chunk) > overlap_sentences:
                        overlap_buffer = current_chunk[-overlap_sentences:]
                        current_chunk = overlap_buffer + [sent]
                        current_length = sum(len(s) for s in current_chunk)
                    else:
                        current_chunk = [sent]
                        current_length = sent_len
        else:
            # í¬ê¸° ì´ˆê³¼ â†’ ë¬´ì¡°ê±´ ìƒˆ ì²­í¬
            chunks.append(' '.join(current_chunk))

            # Overlap ì²˜ë¦¬
            if overlap_sentences > 0 and len(current_chunk) > overlap_sentences:
                overlap_buffer = current_chunk[-overlap_sentences:]
                current_chunk = overlap_buffer + [sent]
                current_length = sum(len(s) for s in current_chunk)
            else:
                current_chunk = [sent]
                current_length = sent_len

    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

def analyze_chunking(original_doc, chunks):
    """ì²­í‚¹ ê²°ê³¼ ë¶„ì„ (ë””ë²„ê¹…ìš©)"""
    print(f"\n{'='*80}")
    print(f"ì²­í‚¹ ë¶„ì„")
    print(f"{'='*80}")
    print(f"ì›ë³¸ ê¸¸ì´: {len(original_doc)}ì")
    print(f"ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")
    print(f"\nê° ì²­í¬:")
    for i, chunk in enumerate(chunks, 1):
        print(f"\n[ì²­í¬ {i}] ({len(chunk)}ì)")
        print(f"{chunk[:100]}...")  # ì²« 100ìë§Œ ì¶œë ¥

def test_semantic_chunking():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("="*80)
    print("Semantic Chunking í…ŒìŠ¤íŠ¸")
    print("="*80)

    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ (DNA + ê´‘í•©ì„± í˜¼ì¬)
    test_doc = """DNAëŠ” ìœ ì „ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ì´ì¤‘ë‚˜ì„  êµ¬ì¡°ì˜ ë¶„ìì´ë‹¤.
DNA ë³µì œëŠ” ë°˜ë³´ì¡´ì  ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë˜ë©°, ê° ê°€ë‹¥ì´ ì£¼í˜• ì—­í• ì„ í•œë‹¤.
DNA ì¤‘í•©íš¨ì†ŒëŠ” ìƒˆë¡œìš´ ê°€ë‹¥ì„ í•©ì„±í•˜ëŠ” í•µì‹¬ íš¨ì†Œì´ë‹¤.
ì‹ë¬¼ì˜ ê´‘í•©ì„±ì€ ë¹› ì—ë„ˆì§€ë¥¼ í™”í•™ ì—ë„ˆì§€ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë‹¤.
ê´‘í•©ì„±ì€ ì—½ë¡ì†Œì—ì„œ ì¼ì–´ë‚˜ë©° ëª…ë°˜ì‘ê³¼ ì•”ë°˜ì‘ìœ¼ë¡œ ë‚˜ë‰œë‹¤.
ëª…ë°˜ì‘ì—ì„œëŠ” ë¬¼ì´ ë¶„í•´ë˜ì–´ ì‚°ì†Œê°€ ë°œìƒí•˜ê³  ATPê°€ ìƒì„±ëœë‹¤."""

    print(f"\nğŸ“„ ì›ë³¸ ë¬¸ì„œ ({len(test_doc)}ì):")
    print(test_doc)

    # ì²­í‚¹ ì‹¤í–‰ (overlap ì—†ìŒ)
    print(f"\n{'='*80}")
    print("í…ŒìŠ¤íŠ¸ 1: Overlap ì—†ìŒ")
    print(f"{'='*80}")
    chunks_no_overlap = semantic_sentence_chunking(test_doc, max_chunk_size=200, overlap_sentences=0)
    analyze_chunking(test_doc, chunks_no_overlap)

    # ì²­í‚¹ ì‹¤í–‰ (overlap 1ë¬¸ì¥)
    print(f"\n{'='*80}")
    print("í…ŒìŠ¤íŠ¸ 2: Overlap 1ë¬¸ì¥")
    print(f"{'='*80}")
    chunks_overlap1 = semantic_sentence_chunking(test_doc, max_chunk_size=200, overlap_sentences=1)
    analyze_chunking(test_doc, chunks_overlap1)

    print(f"\n{'='*80}")
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print(f"{'='*80}")

if __name__ == "__main__":
    test_semantic_chunking()
