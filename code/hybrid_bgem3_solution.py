"""
BGE-M3 ì„ë² ë”©ì„ í™œìš©í•œ Hybrid Search ì†”ë£¨ì…˜
BM25 + Dense Retrieval (BGE-M3) + Solar LLM Reranking
ëª©í‘œ: MAP@3 0.8+ ë‹¬ì„±
"""

import json
import os
import pickle
import numpy as np
from tqdm import tqdm
from elasticsearch import Elasticsearch
from FlagEmbedding import BGEM3FlagModel
from openai import OpenAI

# ES ì—°ê²°
es = Elasticsearch(['http://localhost:9200'])

# Solar API ì´ˆê¸°í™”
upstage_api_key = os.environ.get('UPSTAGE_API_KEY')
client = None
if upstage_api_key:
    client = OpenAI(
        api_key=upstage_api_key,
        base_url="https://api.upstage.ai/v1/solar"
    )

# BGE-M3 ëª¨ë¸ ë¡œë“œ
print("BGE-M3 ëª¨ë¸ ë¡œë“œ ì¤‘...")
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
print("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ì¼ë°˜ ëŒ€í™” ID
SMALLTALK_IDS = {276, 261, 233, 90, 222, 235, 165, 153, 169, 141, 183}

# BGE-M3 ì„ë² ë”© ë¡œë“œ
print("\nBGE-M3 Dense ì„ë² ë”© ë¡œë“œ ì¤‘...")
with open('embeddings_test_bgem3.pkl', 'rb') as f:
    embeddings_dict = pickle.load(f)
print(f"âœ… {len(embeddings_dict)}ê°œ ë¬¸ì„œ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")
print(f"   ì„ë² ë”© ì°¨ì›: {list(embeddings_dict.values())[0].shape[0]}")

def search_bm25(query, top_k=20):
    """BM25 ê²€ìƒ‰"""
    fetch_size = top_k + 5

    response = es.search(
        index='test',
        body={
            'query': {
                'match': {
                    'content': {
                        'query': query,
                        'analyzer': 'nori'
                    }
                }
            },
            'size': fetch_size
        }
    )

    if not response['hits']['hits']:
        return []

    # original_docid ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    seen_original_docids = set()
    results = []

    for hit in response['hits']['hits']:
        source = hit['_source']
        original_docid = source.get('original_docid', source['docid'])

        if original_docid in seen_original_docids:
            continue

        seen_original_docids.add(original_docid)
        results.append({
            'docid': original_docid,
            'content': source['content'],
            'score': hit['_score'],
            'source': 'bm25'
        })

        if len(results) >= top_k:
            break

    return results

def search_dense_bgem3(query, embeddings_dict, top_k=20):
    """BGE-M3 Dense Retrieval ê²€ìƒ‰"""
    # BGE-M3ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = model.encode(
        [query],
        return_dense=True,
        return_sparse=False,
        return_colbert_vecs=False,
        max_length=512
    )
    query_emb = query_embedding['dense_vecs'][0]

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    scores = []
    for docid, doc_emb in embeddings_dict.items():
        similarity = np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))
        scores.append((docid, similarity))

    # ì •ë ¬
    scores.sort(key=lambda x: x[1], reverse=True)

    # ESì—ì„œ content ê°€ì ¸ì˜¤ê¸°
    results = []
    for docid, score in scores[:top_k]:
        try:
            # ESì—ì„œ ë¬¸ì„œ ì¡°íšŒ
            resp = es.search(
                index='test',
                body={
                    'query': {
                        'bool': {
                            'should': [
                                {'term': {'docid.keyword': docid}},
                                {'term': {'original_docid.keyword': docid}}
                            ]
                        }
                    },
                    'size': 1
                }
            )

            if resp['hits']['hits']:
                source = resp['hits']['hits'][0]['_source']
                results.append({
                    'docid': docid,
                    'content': source['content'],
                    'score': float(score),
                    'source': 'dense_bgem3'
                })
        except Exception as e:
            continue

    return results

def hybrid_search_rrf(query, embeddings_dict, top_k=20, k=60):
    """
    Reciprocal Rank Fusion (RRF)ë¡œ BM25 + Dense (BGE-M3) ê²°í•©

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        embeddings_dict: BGE-M3 ì„ë² ë”© ë”•ì…”ë„ˆë¦¬
        top_k: ê° ê²€ìƒ‰ì—ì„œ ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜
        k: RRF íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’ 60)

    Returns:
        RRFë¡œ ìœµí•©ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    # BM25 ê²€ìƒ‰
    bm25_results = search_bm25(query, top_k=top_k)

    # Dense ê²€ìƒ‰ (BGE-M3)
    dense_results = search_dense_bgem3(query, embeddings_dict, top_k=top_k)

    # RRF ìŠ¤ì½”ì–´ ê³„ì‚°
    rrf_scores = {}
    doc_contents = {}

    # BM25 ê²°ê³¼
    for rank, doc in enumerate(bm25_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + 1 / (k + rank)
        doc_contents[docid] = doc['content']

    # Dense ê²°ê³¼
    for rank, doc in enumerate(dense_results, 1):
        docid = doc['docid']
        rrf_scores[docid] = rrf_scores.get(docid, 0) + 1 / (k + rank)
        doc_contents[docid] = doc['content']

    # ìŠ¤ì½”ì–´ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)

    # ê²°ê³¼ ìƒì„±
    results = []
    for docid, score in sorted_docs:
        results.append({
            'docid': docid,
            'content': doc_contents[docid],
            'score': score
        })

    return results

def llm_rerank(query, docs, top_k=3):
    """Solar-proë¡œ Reranking"""
    if not docs or len(docs) <= top_k or not client:
        return [doc['docid'] for doc in docs[:top_k]]

    try:
        # ë¬¸ì„œ ëª©ë¡ ìƒì„±
        doc_list = []
        for i, doc in enumerate(docs[:15]):  # ìƒìœ„ 15ê°œë§Œ í‰ê°€
            content_preview = doc['content'][:300]
            if len(doc['content']) > 300:
                content_preview += "..."
            doc_list.append(f"[{i}] {content_preview}")

        docs_text = "\n\n".join(doc_list)

        response = client.chat.completions.create(
            model="solar-pro",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ê³¼í•™ ì§€ì‹ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ relevance íŒë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì •í™•íˆ ì„ íƒí•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": f"""ì¿¼ë¦¬: {query}

ë¬¸ì„œë“¤:
{docs_text}

ì´ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ {top_k}ê°œì˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.
- ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ”ê°€?
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ”ê°€?
- ê³¼í•™ì ìœ¼ë¡œ ì •í™•í•œê°€?

ì¶œë ¥: ë²ˆí˜¸ë§Œ ì½¤ë§ˆë¡œ êµ¬ë¶„ (ì˜ˆ: 0,2,4)
ì„¤ëª… ì—†ì´ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
                }
            ],
            temperature=0.0,
            max_tokens=30
        )

        result = response.choices[0].message.content.strip()
        indices = [int(x.strip()) for x in result.split(',') if x.strip().isdigit()]

        reranked_docids = []
        for idx in indices[:top_k]:
            if 0 <= idx < len(docs):
                reranked_docids.append(docs[idx]['docid'])

        # ë¶€ì¡±í•˜ë©´ ì›ë˜ ìˆœì„œë¡œ ì±„ìš°ê¸°
        for doc in docs:
            if len(reranked_docids) >= top_k:
                break
            if doc['docid'] not in reranked_docids:
                reranked_docids.append(doc['docid'])

        return reranked_docids[:top_k]

    except Exception as e:
        print(f"âš ï¸  Reranking ì‹¤íŒ¨: {e}")
        return [doc['docid'] for doc in docs[:top_k]]

def hybrid_bgem3_strategy(eval_id, query, embeddings_dict):
    """BGE-M3 ê¸°ë°˜ Hybrid ì „ëµ"""
    # ì¼ë°˜ ëŒ€í™”ëŠ” ë¹ˆ ê²°ê³¼
    if eval_id in SMALLTALK_IDS:
        return []

    # Hybrid Search (RRF)
    hybrid_results = hybrid_search_rrf(query, embeddings_dict, top_k=20, k=60)

    if not hybrid_results:
        return []

    # LLM Reranking
    final_topk = llm_rerank(query, hybrid_results, top_k=3)

    return final_topk

def run_bgem3_experiment():
    """BGE-M3 ê¸°ë°˜ ì‹¤í—˜ ì‹¤í–‰"""
    print("="*80)
    print("BGE-M3 Hybrid Search ì‹¤í—˜")
    print("="*80)
    print("ì „ëµ: BM25 + Dense (BGE-M3, 1024d) + Solar LLM Reranking")
    print("ê°œì„ : ko-sroberta (768d) â†’ BGE-M3 (1024d)")
    print("="*80)

    # Eval ë°ì´í„° ë¡œë“œ
    with open('../data/eval.jsonl', 'r', encoding='utf-8') as f:
        eval_data = [json.loads(line) for line in f]

    print(f"\nğŸ“‹ ì´ {len(eval_data)}ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘\n")

    results = []

    for item in tqdm(eval_data, desc="BGE-M3 Hybrid Search"):
        eval_id = item['eval_id']

        # ì¿¼ë¦¬ ì¶”ì¶œ
        if isinstance(item['msg'], list):
            query = item['msg'][-1]['content']
        else:
            query = item['msg']

        # Hybrid ì „ëµ ì‹¤í–‰
        topk = hybrid_bgem3_strategy(eval_id, query, embeddings_dict)

        results.append({
            'eval_id': eval_id,
            'retrieve': topk
        })

    # ì œì¶œ íŒŒì¼ ìƒì„±
    output_path = 'hybrid_bgem3_submission.csv'
    with open(output_path, 'w', encoding='utf-8') as f:
        for r in results:
            json_obj = {
                'eval_id': r['eval_id'],
                'topk': r['retrieve']
            }
            f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')

    print(f"\n{'='*80}")
    print(f"âœ… ì‹¤í—˜ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ğŸ’¾ ì œì¶œ íŒŒì¼: {output_path}")
    print(f"{'='*80}")

    return results

if __name__ == "__main__":
    run_bgem3_experiment()
