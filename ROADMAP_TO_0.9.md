# ğŸ¯ MAP@3 0.9 ë‹¬ì„± ë¡œë“œë§µ

## í˜„ì¬ ìƒí™©

- **í˜„ì¬ ìµœê³  ì ìˆ˜**: **0.8030** ğŸ†
- **ëª©í‘œ ì ìˆ˜**: 0.9
- **í•„ìš”í•œ í–¥ìƒ**: +0.097 (+12.1%)
- **ë² ì´ìŠ¤ë¼ì¸**: 0.7848

### ì„±ëŠ¥ í–¥ìƒ ì—¬ì •

```
0.7848 (Baseline)
  â†“ +1.16%
0.7939 (cascaded_reranking_v1 Previous)
  â†“ +1.15%
0.8030 (cascaded_reranking_v1 Final) ğŸ† â† í˜„ì¬ ìœ„ì¹˜
  â†“ +12.1% (ëª©í‘œ)
0.9000 (Target) ğŸ¯
```

---

## ğŸ“Š ë³‘ëª© ë¶„ì„ ê²°ê³¼

### Ultra Validation Set ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„ (7ê°œ)

| ë‹¨ê³„ | ì‹¤íŒ¨ ê°œìˆ˜ | ë¹„ìœ¨ | ì‹¬ê°ë„ |
|------|----------|------|--------|
| **Retrieval (ì´ˆê¸° ê²€ìƒ‰)** | 6ê°œ | 85.7% | ğŸ”´ **HIGH** |
| Reranking (ì¬ì •ë ¬) | 1ê°œ | 14.3% | ğŸŸ¢ LOW |

**í•µì‹¬ ë°œê²¬**:
- **Retrieval Recallì´ ë³‘ëª©**: Top-30ì— ì •ë‹µì´ ì—†ìœ¼ë©´ Rerankingë„ ë¬´ìš©ì§€ë¬¼
- Rerankingì€ ì´ë¯¸ ì˜ ì‘ë™ (6/7 ì„±ê³µë¥ )
- **ìš°ì„ ìˆœìœ„**: Retrieval ê°œì„  >> Reranking ê°œì„ 

### Retrieval ì‹¤íŒ¨ ì‚¬ë¡€ (6ê°œ)

**íŒ¨í„´ ë¶„ì„**:
1. **í¬ê·€ ìš©ì–´/ê³ ìœ ëª…ì‚¬**: 3ê°œ
   - "í”Œë‘í¬í†¤ì˜ ì—­í• ", "interferon", "bridge inverter"
2. **ì¶”ìƒì  í‘œí˜„**: 2ê°œ
   - "ë‹¬ì´ í•­ìƒ ê°™ì€ ë©´ë§Œ ë³´ì´ëŠ” ì´ìœ "
3. **ë„ë©”ì¸ íŠ¹í™” ìš©ì–´**: 1ê°œ
   - "ì„±ëŒ€ ì£¼ë¦„ ê¸´ì¥"

---

## ğŸš€ 3ë‹¨ê³„ ê°œì„  ì „ëµ

### Phase 1: BM25 íŒŒë¼ë¯¸í„° íŠœë‹ (ì˜ˆìƒ +2~5%) ğŸ”´ ìµœìš°ì„ 

**í˜„ì¬ ìƒíƒœ**:
```python
# Elasticsearch ê¸°ë³¸ê°’ ì‚¬ìš©
k1 = 1.2  # Term frequency saturation
b = 0.75  # Length normalization
```

**ë¬¸ì œì **:
- í•œêµ­ì–´ ê³¼í•™ ë¬¸ì„œëŠ” ì¼ë°˜ ë¬¸ì„œë³´ë‹¤ ê¸¸ì´ í¸ì°¨ê°€ í¼
- ê¸°ë³¸ íŒŒë¼ë¯¸í„°ëŠ” ì˜ì–´ ì›¹ ë¬¸ì„œì— ìµœì í™”ë¨

**í•´ê²°ì±…**:
```python
# Grid Searchë¡œ ìµœì ê°’ ì°¾ê¸°
for k1 in [0.8, 1.0, 1.2, 1.5, 2.0]:
    for b in [0.0, 0.25, 0.5, 0.75, 1.0]:
        map_score = evaluate(k1, b)
```

**ì˜ˆìƒ íš¨ê³¼**:
- MAP@3 0.8030 â†’ 0.82~0.84 (+2~5%)
- ì‹¤í–‰ ì‹œê°„: 1~2ì‹œê°„

**êµ¬í˜„ ê³„íš**:
1. `bm25_parameter_tuning.py` ì‘ì„±
2. Ultra Validation Setìœ¼ë¡œ í‰ê°€
3. ìµœì  íŒŒë¼ë¯¸í„° ì„ íƒ
4. ì „ì²´ ë°ì´í„°ì…‹ ì œì¶œ

---

### Phase 2: Hybrid Weight ìµœì í™” (ì˜ˆìƒ +1~3%) ğŸŸ¡

**í˜„ì¬ ìƒíƒœ**:
```python
# RRF Fusion (k=60)
# BM25ì™€ BGE-M3 ë™ë“± ê°€ì¤‘ì¹˜
```

**ë¬¸ì œì **:
- BM25ì™€ Denseì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ ë¯¸ì¡°ì •
- RRF k ê°’ì´ ìµœì ì´ ì•„ë‹ ìˆ˜ ìˆìŒ

**í•´ê²°ì±…**:
```python
# Weighted Hybrid
final_score = alpha * bm25_score + (1-alpha) * dense_score

# Grid Search
for alpha in [0.5, 0.6, 0.7, 0.8, 0.9]:
    for rrf_k in [30, 60, 90, 120]:
        map_score = evaluate(alpha, rrf_k)
```

**ì˜ˆìƒ íš¨ê³¼**:
- MAP@3 0.84 â†’ 0.85~0.86 (+1~3%)
- BM25 ê°€ì¤‘ì¹˜ ë†’ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒ (alpha=0.7~0.8)

**êµ¬í˜„ ê³„íš**:
1. `hybrid_weight_tuning.py` ì‘ì„±
2. Ultra Validation Setìœ¼ë¡œ í‰ê°€
3. ìµœì  ê°€ì¤‘ì¹˜ ì„ íƒ

---

### Phase 3: BGE-M3 Fine-tuning (ì˜ˆìƒ +3~7%) ğŸŸ¢

**í˜„ì¬ ìƒíƒœ**:
```python
# Pre-trained BGE-M3 ì‚¬ìš©
# ì¼ë°˜ ë„ë©”ì¸ í•™ìŠµ ëª¨ë¸
```

**ë¬¸ì œì **:
- ê³¼í•™ ë„ë©”ì¸ íŠ¹í™” í•™ìŠµ ì•ˆ ë¨
- í•œêµ­ì–´ ê³¼í•™ ìš©ì–´ ì„ë² ë”© í’ˆì§ˆ ë‚®ìŒ

**í•´ê²°ì±…**:
```python
# 1. Pseudo-labelingìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„±
# BM25 high-confidence ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìˆ˜ì§‘

# 2. Contrastive Learning
triplets = [
    (query, positive_doc, negative_doc)
    for each training sample
]

# 3. Fine-tuning
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('BAAI/bge-m3')
model.fit(triplets, epochs=3)
```

**ì˜ˆìƒ íš¨ê³¼**:
- MAP@3 0.86 â†’ 0.89~0.90 (+3~7%)
- ê³¼í•™ ìš©ì–´ ì„ë² ë”© í’ˆì§ˆ ëŒ€í­ í–¥ìƒ

**êµ¬í˜„ ê³„íš**:
1. `create_training_data.py` - BM25 ê¸°ë°˜ pseudo-labeling
2. `finetune_bgem3.py` - Fine-tuning
3. `create_embeddings_finetuned.py` - ì¬ìƒì„±
4. ì „ì²´ ë°ì´í„°ì…‹ ì œì¶œ

---

## ğŸ“ˆ ì˜ˆìƒ ìµœì¢… ì ìˆ˜

| Phase | ê°œì„  ë‚´ìš© | ì˜ˆìƒ í–¥ìƒ | ëˆ„ì  ì ìˆ˜ | ë‚œì´ë„ |
|-------|----------|-----------|----------|--------|
| **í˜„ì¬** | cascaded_reranking_v1 Final | - | **0.8030** | - |
| **Phase 1** | BM25 íŒŒë¼ë¯¸í„° íŠœë‹ | +2~5% | **0.82~0.84** | ğŸŸ¢ LOW |
| **Phase 2** | Hybrid Weight ìµœì í™” | +1~3% | **0.83~0.86** | ğŸŸ¢ LOW |
| **Phase 3** | BGE-M3 Fine-tuning | +3~7% | **0.86~0.90** âœ… | ğŸ”´ HIGH |

**ì´ ì˜ˆìƒ í–¥ìƒ**: +6~15% (+0.05~0.12)
**ëª©í‘œ ë‹¬ì„± ê°€ëŠ¥ì„±**: âœ… **HIGH**

---

## ğŸ› ï¸ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš

### Step 1: BM25 íŒŒë¼ë¯¸í„° íŠœë‹ (1~2ì¼)

```bash
cd code

# 1. íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
cat > bm25_parameter_tuning.py << 'EOF'
# Grid Search for BM25 parameters
# Ultra Validation Setìœ¼ë¡œ í‰ê°€
EOF

# 2. ì‹¤í–‰
python3 bm25_parameter_tuning.py

# 3. ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
# index_documents_nori.py ìˆ˜ì •
```

**ê¸°ëŒ€ ê²°ê³¼**:
- ìµœì  k1, b ê°’ ë°œê²¬
- Validation MAP: 0.8030 â†’ 0.82~0.84

---

### Step 2: Hybrid Weight ìµœì í™” (1ì¼)

```bash
# 1. íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
python3 hybrid_weight_tuning.py

# 2. ìµœì  ê°€ì¤‘ì¹˜ ì ìš©
# cascaded_reranking_v1.py ìˆ˜ì •
```

**ê¸°ëŒ€ ê²°ê³¼**:
- ìµœì  alpha, rrf_k ê°’ ë°œê²¬
- Validation MAP: 0.84 â†’ 0.85~0.86

---

### Step 3: BGE-M3 Fine-tuning (3~5ì¼)

```bash
# 1. í•™ìŠµ ë°ì´í„° ìƒì„±
python3 create_training_data.py
# ì¶œë ¥: training_triplets.json (ì˜ˆìƒ 1000~2000 ìŒ)

# 2. Fine-tuning
python3 finetune_bgem3.py
# ì†Œìš” ì‹œê°„: 2~4ì‹œê°„ (GPU í•„ìš”)

# 3. ì„ë² ë”© ì¬ìƒì„±
python3 create_embeddings_finetuned.py

# 4. ì œì¶œ íŒŒì¼ ìƒì„±
python3 generate_full_submission.py
```

**ê¸°ëŒ€ ê²°ê³¼**:
- ê³¼í•™ ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”©
- Validation MAP: 0.86 â†’ 0.89~0.90

---

## ğŸ’¡ ì¶”ê°€ ìµœì í™” ì•„ì´ë””ì–´

### 1. Prompt Engineering (ì˜ˆìƒ +1~2%)

**í˜„ì¬ Reranking Prompt**:
```python
prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œê°€ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
ì§ˆë¬¸: {query}
ë¬¸ì„œ: {doc}
"""
```

**ê°œì„  ë°©í–¥**:
```python
# 1. ê³¼í•™ ë„ë©”ì¸ íŠ¹í™”
# 2. Few-shot examples ì¶”ê°€
# 3. Chain-of-Thought reasoning
```

### 2. Query Expansion (ì˜ˆìƒ +1~2%)

```python
# ì˜ì–´ í‚¤ì›Œë“œ í•œê¸€ ë³€í™˜
"interferon" â†’ "ì¸í„°í˜ë¡ "

# ë™ì˜ì–´ í™•ì¥
"ì—­í• " â†’ ["ì—­í• ", "ê¸°ëŠ¥", "ì‘ìš©", "íš¨ê³¼"]
```

### 3. Semantic Chunking ì¬ì‹œë„ (ì˜ˆìƒ +2~4%)

**ì´ì „ ì‹¤íŒ¨ ì›ì¸**:
- Task 6ì—ì„œ ë°ì´í„° êµ¬ì¡° í•œê³„ë¡œ í¬ê¸°

**ìƒˆë¡œìš´ ì ‘ê·¼**:
```python
# Chunk ë‹¨ìœ„ ê²€ìƒ‰ + Full Document ì¬êµ¬ì„±
1. Chunk ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
2. Chunkê°€ ì†í•œ Full Document ë°˜í™˜
3. LLM Rerankingìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
```

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: BM25 íŠœë‹ (í•„ìˆ˜) âœ… ì™„ë£Œ
- [x] bm25_parameter_tuning.py ì‘ì„± â†’ `optimize_bm25.py`
- [x] Ultra Validation Setìœ¼ë¡œ í‰ê°€
- [x] ìµœì  íŒŒë¼ë¯¸í„° ì„ íƒ (k1, b) â†’ **k1=0.9, b=0.5 (MAP@3 0.99)**
- [x] index_documents_nori.py ì—…ë°ì´íŠ¸ â†’ **BM25 k1=0.9, b=0.5 ì ìš© ì™„ë£Œ**
- [x] ì „ì²´ ì¸ë±ìŠ¤ ì¬ìƒì„± ì™„ë£Œ (4272ê°œ ë¬¸ì„œ)
- [x] ì „ì²´ ë°ì´í„°ì…‹ ì œì¶œ â†’ **`cascaded_reranking_v1_full_submission_20251124_201646.csv`**

### Phase 2: Hybrid Weight ìµœì í™” (ê¶Œì¥) âœ… ì™„ë£Œ
- [x] hybrid_weight_tuning.py ì‘ì„± âœ…
- [x] Grid Search ì‹¤í–‰ (alpha, rrf_k) â†’ **k=30 (MAP@3 0.99)**
- [x] cascaded_reranking_v1.py í™•ì¸ â†’ **ì´ë¯¸ k=30 ì‚¬ìš© ì¤‘** âœ…
- [x] ì „ì²´ ë°ì´í„°ì…‹ ì œì¶œ â†’ **`cascaded_reranking_v1_full_submission_20251124_201646.csv`**

### Phase 3: BGE-M3 Fine-tuning (ì„ íƒ)
- [ ] create_training_data.py ì‘ì„±
- [ ] Pseudo-labelingìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„±
- [ ] finetune_bgem3.py ì‘ì„±
- [ ] GPU í™˜ê²½ í™•ë³´
- [ ] Fine-tuning ì‹¤í–‰ (2~4ì‹œê°„)
- [ ] ì„ë² ë”© ì¬ìƒì„±
- [ ] ì „ì²´ ë°ì´í„°ì…‹ ì œì¶œ

### ì¶”ê°€ ìµœì í™” (ì„ íƒ)
- [ ] Prompt Engineering
- [ ] Query Expansion
- [ ] Semantic Chunking ì¬ì‹œë„

---

## ğŸ¯ í•µì‹¬ ë©”ì‹œì§€

**0.8030 â†’ 0.9 ë‹¬ì„±ì€ ì¶©ë¶„íˆ ê°€ëŠ¥í•©ë‹ˆë‹¤!**

### ì„±ê³µ í™•ë¥ 

| ì‹œë‚˜ë¦¬ì˜¤ | ì˜ˆìƒ ì ìˆ˜ | í™•ë¥  |
|---------|----------|------|
| Phase 1ë§Œ ì™„ë£Œ | **0.82~0.84** | 90% |
| Phase 1+2 ì™„ë£Œ | **0.85~0.86** | 80% |
| Phase 1+2+3 ì™„ë£Œ | **0.89~0.90** âœ… | 70% |

### ì„±ê³µ ìš”ì¸

1. **BM25 íŒŒë¼ë¯¸í„°ê°€ ìµœì í™”ë˜ì§€ ì•ŠìŒ** (ê°€ì¥ ì‰¬ìš´ ê°œì„ )
2. **Hybrid Weightê°€ ì¡°ì •ë˜ì§€ ì•ŠìŒ** (ë¹ ë¥¸ ê°œì„ )
3. **BGE-M3ì´ ê³¼í•™ ë„ë©”ì¸ í•™ìŠµ ì•ˆ ë¨** (í° ê°œì„  ì—¬ì§€)

### ë¦¬ìŠ¤í¬

1. **Phase 3 GPU í•„ìš”**: Colab Pro ë˜ëŠ” AWS ì‚¬ìš©
2. **Fine-tuning ì‹¤íŒ¨ ê°€ëŠ¥ì„±**: Hyperparameter ì¡°ì • í•„ìš”
3. **Overfitting ìœ„í—˜**: Ultra Validation Set í¬ê¸° ì‘ìŒ (8ê°œ)

---

## ğŸ“ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—…

### 1. BM25 íŒŒë¼ë¯¸í„° íŠœë‹ (ì˜¤ëŠ˜ ì‹œì‘ ê°€ëŠ¥)

```bash
cd code
python3 bm25_parameter_tuning.py
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1~2ì‹œê°„
**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: +2~5% (0.8030 â†’ 0.82~0.84)

### 2. ë¬¸ì„œ ì½ê¸°

**í•„ìˆ˜ ë¬¸ì„œ**:
- [EXPERIMENT_SUMMARY_20251124.md](code/EXPERIMENT_SUMMARY_20251124.md) - ì „ì²´ ì‹¤í—˜ ê³¼ì •
- [code/docs/TASK5_FAILURE_ANALYSIS.md](code/docs/TASK5_FAILURE_ANALYSIS.md) - ì‹¤íŒ¨ ë¶„ì„

**ì°¸ê³  ë¬¸ì„œ**:
- [Elasticsearch BM25 Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html)
- [BGE-M3 Fine-tuning Guide](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune)

---

## ğŸ”¬ ì‹¤í—˜ ìš°ì„ ìˆœìœ„

| ìˆœìœ„ | ì‹¤í—˜ | ë‚œì´ë„ | ì˜ˆìƒ í–¥ìƒ | ROI |
|------|------|--------|----------|-----|
| 1 | BM25 íŒŒë¼ë¯¸í„° íŠœë‹ | ğŸŸ¢ LOW | +2~5% | â­â­â­â­â­ |
| 2 | Hybrid Weight ìµœì í™” | ğŸŸ¢ LOW | +1~3% | â­â­â­â­ |
| 3 | Prompt Engineering | ğŸŸ¡ MED | +1~2% | â­â­â­ |
| 4 | Query Expansion | ğŸŸ¡ MED | +1~2% | â­â­â­ |
| 5 | BGE-M3 Fine-tuning | ğŸ”´ HIGH | +3~7% | â­â­â­â­â­ |
| 6 | Semantic Chunking | ğŸ”´ HIGH | +2~4% | â­â­ |

**ì¶”ì²œ ìˆœì„œ**: 1 â†’ 2 â†’ 5 (Phase 1 â†’ 2 â†’ 3)

---

## ğŸ“… íƒ€ì„ë¼ì¸

### Week 1-2 (í˜„ì¬)
- [x] Task 7 ì™„ë£Œ: MAP@3 0.8030 ë‹¬ì„±
- [x] ì‹¤í—˜ ê²°ê³¼ ë¬¸ì„œí™”
- [x] GitHub í‘¸ì‹œ ì™„ë£Œ
- [ ] BM25 íŒŒë¼ë¯¸í„° íŠœë‹

### Week 3-4
- [ ] Hybrid Weight ìµœì í™”
- [ ] Prompt Engineering
- [ ] Query Expansion

### Week 5-6 (ì„ íƒ)
- [ ] BGE-M3 Fine-tuning
- [ ] Semantic Chunking ì¬ì‹œë„
- [ ] ì•™ìƒë¸” ë°©ë²• ì‹œë„

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-24
**í˜„ì¬ ìµœê³  ì„±ëŠ¥**: MAP@3 **0.8030** ğŸ†
**ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤**: MAP@3 **0.85** (Phase 1+2 ì™„ë£Œ)
