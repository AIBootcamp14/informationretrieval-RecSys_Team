# 평가방법 정리 (howtoeval.md)

본 문서는 *Scientific Knowledge Question Answering \| 과학 지식 질의
응답 시스템 구축* 대회의 **평가방법(Evaluation Method)**을 상세히 정리한
것입니다.\
대회에서는 **정확한 RAG 모델의 End-to-End 성능**이 아니라, **검색된
문서의 적합도(relevance)**를 중심으로 성능을 평가합니다.

------------------------------------------------------------------------

## 1. 평가 목적

사용자가 입력한 질문이 과학 지식 기반인지 여부와 관계없이,\
모델이 **정확한 레퍼런스 문서(Document)를 얼마나 잘 검색했는지**를
자동으로 측정하기 위함입니다.

문장 생성 품질은 사람에 따라 다양하게 판단될 수 있어 자동화가 어렵기
때문에,\
본 대회는 **문서 검색력 자체**를 평가 대상으로 삼습니다.

------------------------------------------------------------------------

## 2. 사용되는 평가 지표: MAP (Mean Average Precision)

### ● MAP란?

-   **N개의 질의(Query)**에 대해 각각 **Average Precision(AP)**을 계산한
    뒤,
-   그 평균값을 취한 값입니다.
-   AP는 "**Precision-Recall Curve** 아래 면적"으로 이해할 수 있습니다.

즉,\
\> "관련 문서를 얼마나 상위에 잘 배치했는가?"\
를 정량적으로 평가합니다.

------------------------------------------------------------------------

## 3. MAP 계산 방식 (일반적인 AP 계산 과정)

1.  Query마다 반환된 문서를 순서대로 하나씩 보며,
2.  i번째 위치에서 **정답 문서(ground truth)**를 찾으면 `hit_count`
    증가,
3.  해당 위치에서의 Precision 계산 → `hit_count / (i+1)`,
4.  모든 정답 문서에 대한 Precision의 평균이 그 Query의 AP,
5.  전체 Query의 평균이 MAP.

------------------------------------------------------------------------

## 4. 본 대회에서의 MAP 변형

### 이유

-   대회 데이터에는 **과학 지식 질문이 아닌 질의**도 포함됩니다.
-   이 경우에는 검색 자체가 필요하지 않습니다.

### 적용되는 로직

-   Ground truth가 "검색 불필요"로 표시된 경우 (`gt[eval_id]`가
    비어있음):
    -   **검색 결과가 비어 있음 → 1점**
    -   **검색 결과가 존재함 → 0점**

즉, \> 검색할 필요 없는 질문인데도 문서를 반환하면 감점\
\> 검색 필요 없는 질문에서 아무 문서도 반환하지 않으면 만점

------------------------------------------------------------------------

## 5. 공식 평가 코드 설명

``` python
def calc_map(gt, pred):
    sum_average_precision = 0

    for j in pred:
        # 1) 검색이 필요한 Query
        if gt[j["eval_id"]]:
            hit_count = 0
            sum_precision = 0

            # top-3 문서만 평가
            for i, docid in enumerate(j["topk"][:3]):
                if docid in gt[j["eval_id"]]:
                    hit_count += 1
                    sum_precision += hit_count/(i+1)

            # AP 계산
            average_precision = sum_precision / hit_count if hit_count > 0 else 0

        # 2) 검색이 필요 없는 Query
        else:
            average_precision = 0 if j["topk"] else 1

        sum_average_precision += average_precision

    return sum_average_precision / len(pred)
```

### 핵심 요약

-   **top-3 문서만 평가한다.**
-   **검색 필요 여부에 따라 평가 방식이 달라진다.**
-   **검색 필요 없는 Query → 문서 반환하면 0점, 미반환이면 1점**

------------------------------------------------------------------------

## 6. 제출 방법

-   제공된 `data.tar.gz` 내의 **eval.jsonl**이 평가 입력입니다.
-   이를 기반으로 검색 결과(prediction)를 생성하여 제출합니다.
-   제출한 JSON 결과는 자동으로 위 MAP 기준에 따라 리더보드에
    반영됩니다.

------------------------------------------------------------------------

## 7. 요약

  항목                  설명
  --------------------- ---------------------------------
  평가 지표             MAP (변형 MAP)
  평가 대상             검색된 문서의 적합도
  topk 평가             top-3
  검색 필요 없는 질의   문서 미반환 시 1점, 반환 시 0점
  최종 점수             모든 Query의 AP 평균
